<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.jpeg?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.jpeg?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.jpeg?v=5.1.4">


  <link rel="mask-icon" href="/images/favicon.jpeg?v=5.1.4" color="#222">





  <meta name="keywords" content="Attention,Survey,Deep Learning,Neural Networks,">










<meta name="description" content="《Attention, please! A survey of Neural Attention Models in Deep Learning》为2021年新发表的详细介绍Attention机制的大综述文章，其全文文本共66页，引用了569篇文章，可以说是几乎覆盖了机器学习领域所有可圈点的Attention相关文章。其涵盖了NLP、CV、推荐、强化学习等可以说是各个机器学习覆盖等领域（当然">
<meta name="keywords" content="Attention,Survey,Deep Learning,Neural Networks">
<meta property="og:type" content="article">
<meta property="og:title" content="【Paper-Reading】《Attention, please! A survey of Neural Attention Models in Deep Learning》">
<meta property="og:url" content="https://liuzhiqi.github.io/blog/2021/04/02/【Paper-Reading】ATTENTION-PLEASE-A-SURVEY-OF-NEURAL-ATTENTION-MODELS-IN-DEEP-LEARNING/index.html">
<meta property="og:site_name" content="Zhiqi Liu">
<meta property="og:description" content="《Attention, please! A survey of Neural Attention Models in Deep Learning》为2021年新发表的详细介绍Attention机制的大综述文章，其全文文本共66页，引用了569篇文章，可以说是几乎覆盖了机器学习领域所有可圈点的Attention相关文章。其涵盖了NLP、CV、推荐、强化学习等可以说是各个机器学习覆盖等领域（当然">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//cite_bubble.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//RNNSearch.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//BiDAF.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//HAN.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//FusionNet.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//FusionNet2.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//Rocktaschel_et_al_%5B57%5D.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//RAM.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//DRAW.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//GCA-LSTM.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//GCA-LSTM_train.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//AConvLSTM.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//Hori_Attention-Based_Multimodal_Fusion_ICCV_2017.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//AAAI2018_77.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//Relational_recurrent_neural_networks.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//Relational_recurrent_neural_networks_eq.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//NMBT.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//DAN_eq_p.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//DAN_eq_t.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//DAN.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//Bi-directional_attention.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//LSTM-P.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//CVPR2019_85.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//NTM.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//Memory_Networks.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//E2EMemNet.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//Space-Time_Memory_Networks.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//DMN.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//KV-MemNN.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//MemGNN.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//Episodic_Graph_Mem_Net.jpg">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//Transformer.jpg">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//Transformer_evo.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//Star-transformer.jpg">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//Sparse_Transformers.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//Input-Combination-Strategies-for-Multi-Source-Transformer-Decoder.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//HighWay_Recurrent_Transformer.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//TTS_Transformer.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//BERT1.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//top-down_bottom-up.png">
<meta property="og:image" content="https://liuzhiqi.github.io/.io//mattnet.jpeg">
<meta property="og:updated_time" content="2021-06-03T15:02:45.704Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="【Paper-Reading】《Attention, please! A survey of Neural Attention Models in Deep Learning》">
<meta name="twitter:description" content="《Attention, please! A survey of Neural Attention Models in Deep Learning》为2021年新发表的详细介绍Attention机制的大综述文章，其全文文本共66页，引用了569篇文章，可以说是几乎覆盖了机器学习领域所有可圈点的Attention相关文章。其涵盖了NLP、CV、推荐、强化学习等可以说是各个机器学习覆盖等领域（当然">
<meta name="twitter:image" content="https://liuzhiqi.github.io/.io//cite_bubble.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://liuzhiqi.github.io/blog/2021/04/02/【Paper-Reading】ATTENTION-PLEASE-A-SURVEY-OF-NEURAL-ATTENTION-MODELS-IN-DEEP-LEARNING/">





  <title>【Paper-Reading】《Attention, please! A survey of Neural Attention Models in Deep Learning》 | Zhiqi Liu</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Zhiqi Liu</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-articles">
          <a href="/articles" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-book"></i> <br>
            
            Articles
          </a>
        </li>
      
        
        <li class="menu-item menu-item-notes">
          <a href="/categories/Notes" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-wpforms"></i> <br>
            
            Notes
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-notes" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://liuzhiqi.github.io/blog/2021/04/02/【Paper-Reading】ATTENTION-PLEASE-A-SURVEY-OF-NEURAL-ATTENTION-MODELS-IN-DEEP-LEARNING/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhiqi Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhiqi Liu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">【Paper-Reading】《Attention, please! A survey of Neural Attention Models in Deep Learning》</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-04-02T22:13:11+08:00">
                2021-04-02
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Notes/" itemprop="url" rel="index">
                    <span itemprop="name">Notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<hr>
<p>《Attention, please! A survey of Neural Attention Models in Deep Learning》为2021年新发表的详细介绍Attention机制的大综述文章，其全文文本共66页，引用了569篇文章，可以说是几乎覆盖了机器学习领域所有可圈点的Attention相关文章。其涵盖了NLP、CV、推荐、强化学习等可以说是各个机器学习覆盖等领域（当然这也是因为Attention本身有效与普适）。<br>文章主要分为了：1 Introduction、2 Overview、3 Attention Mechanisms、4 Attention-based Classic Deep Learning Architectures、5 Applications、6 Trends and Opportunities、7 Conclusions共7节，其中前三节主要简述了Attention相关发展、概念与框架，同时也简单介绍了Attention的各种变形及在各个领域的应用。4、5两节主要从架构和应用场景上对Attention进行了分类，并引用了大量的相关文章，这一部分的文章和1、2节略有重复。<br>而本文主要对前三节进行阐述，只是想了解Attention发展或了解Attention的变形和应用同学，到前三节已经足够了。对于后四节，本文作者只是简单浏览并留下了相应标题。如果有感兴趣或研究相应领域的同学可以自行下载文章阅读，后续如果有机会，我可能会对其进行补全。<br>对于前三节作者提到的文章，除了原文作者描述之外，我还加入了更详细阐述和一些个人的理解，由于涉及的论文量比较大，一些文章我也仅仅是快速浏览，如果有理解错误或片面的地方，还请原谅，同时也欢迎大家指出与纠正错误。<br>最后贴一下原文链接： <a href="https://arxiv.org/abs/2103.16775" target="_blank" rel="noopener">《Attention, please! A survey of Neural Attention Models in Deep Learning》</a></p>
<hr>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>最初Attention主要应用于计算机视觉，用于选择图像的重要区域。且在深度学习之前，attention机制一直都有良好的表现，例如用attention进行图像压缩、图像匹配、图像分割、物体捕捉等等。</p>
<p>自2014年，Attention逐渐成为深度学习领域一个基本的概念，关于attention的相关发表数量也逐年递增。在神经网络中，Attention机制被用来管理信息、特征传递，提升学习效果，过滤无关信息，帮助网络处理long-time dependencies等。  </p>
<p><strong>相关Survey：</strong>   </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1. Recurrent networks and applications in computer vision：  </span><br><span class="line">Survey on the attention based rnn model and its applications in computer vision, 2016</span><br><span class="line">2. Attention in natural language processing (NLP)：  </span><br><span class="line">An introductory survey on attention mechanisms in nlp problems. 2019.</span><br><span class="line">Attention in natural language processing, 2020.  </span><br><span class="line">3. Attention in graph neural networks:    </span><br><span class="line">Attention models in graphs: A survey, 2018.  </span><br><span class="line">4. General short review:  </span><br><span class="line">An attentive survey of attention models, 2019</span><br></pre></td></tr></table></figure>

<p>论文原文是从6000多的文章中，找到并着重分析了650篇文章。本文则着重从其中挑选了作者详细提及或个人认为有可取之处的文章进行了略微详细的介绍。其中引用编号保留了原文的编号，详细提及的文章后面link了相应paper的超链接，想仔细了解的同学可以自行下载。</p>
<h1 id="2-Overview"><a href="#2-Overview" class="headerlink" title="2 Overview"></a>2 Overview</h1><p>在一切开始前，首先放一张，Attention相关的，根据论文引用情况得到的Bubble Chart：</p>
<p><img src="/.io//cite_bubble.png" alt="bubble"><br>可以看到引用最多的为：RNNSearch [2015, 44], Transformer [37], Memory Networks [38], “show, attend and tell” [45] 和 RAM [46]。<br>而在这其中，RNNSearch应该是Attention在NlP领域最先提出，或者说在最初阶段最有名的文章，因此首先介绍一下RNNSearch：</p>
<h5 id="RNNSearch：-Neural-machine-translation-by-jointly-learning-to-align-and-translate-2019-115"><a href="#RNNSearch：-Neural-machine-translation-by-jointly-learning-to-align-and-translate-2019-115" class="headerlink" title="RNNSearch： Neural machine translation by jointly learning to align and translate [2019, 115]"></a>RNNSearch： Neural machine translation by jointly learning to align and translate <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">[2019, 115]</a></h5><p>经典的Encoder-Decoder框架的瓶颈问题驱动了Attention的发展。最主要的问题是NN需要将原句中所有信息压缩到一个固定大小的向量，Choetal. [47]指出，经典Encoder-Decoder框架效果，会随着句子的长度变长而快速恶化。因此Bahdanau et al. [44] 提出了RNNSearch，将输入向量转化成一些列的内容向量，在预测某一个词时，利用attention机制对Encode出的向量序列进行加权求和。如图：<br><img src="/.io//RNNSearch.png" alt="bubble"><br>其中，加权求和时，权重与:<br>$$<br>a _{ij} = \frac {e _{ij}}{\sum _{k=1} ^{T _x} exp(e _{ik})} \\<br>e _{ij} = a(s _{i-1},h _j)<br>$$</p>
<p>RNNSearch是一种soft attention方法，基于RNNSearch在NN上有很多的尝试，其中比较有代表性的两个方面的工作为：attentional interfaces 和 end-to-end attention。<br><strong>Attentional interfaces</strong>将attention视作一个模块，因此可以很容易的将其插入经典深度学习网络中，如RNNSearch。<br><strong>End-to-end attention</strong>则是一个比较新的研究方向，其将主要用attention层作为主要层，替换了传统神经网络连接层，将attention覆盖到了整个神经网络。而这种基于attention 的End-to-end模型也因此成为了一种新的神经网络类型。</p>
<h2 id="2-1-Attentional-interfaces"><a href="#2-1-Attentional-interfaces" class="headerlink" title="2.1 Attentional interfaces"></a>2.1 Attentional interfaces</h2><p>RNNSearch是Attentional interfaces研究的基础，在其之上有很多扩展。  </p>
<h3 id="2-1-1-Attentional-interfaces-in-NLP"><a href="#2-1-1-Attentional-interfaces-in-NLP" class="headerlink" title="2.1.1 Attentional interfaces in NLP"></a>2.1.1 Attentional interfaces in NLP</h3><h5 id="Listen-Attend-and-Spell-2015-48"><a href="#Listen-Attend-and-Spell-2015-48" class="headerlink" title="Listen, Attend and Spell [2015, 48]:"></a>Listen, Attend and Spell <a href="pdfs.semanticscholar.org/45f2/c264c496296b6600fafc340b8f0dbc4ac52f.pdf">[2015, 48]</a>:</h5><p>采用了类似RNNSearch的模型，将RNN+Attention扩展到了语音识别上（Encode编码语音，Decode解码成文本），RNN部分使用，Attention部分和RNNSearch类似，其中\(e _{ij} = &lt;\phi(s _i),\psi(h _u)&gt;\)，其使用MLP将语音编码和文本状态映射到同一空间上，用内积计算相似度。</p>
<h5 id="Grammar-as-a-Foreign-Language-2015-49"><a href="#Grammar-as-a-Foreign-Language-2015-49" class="headerlink" title="Grammar as a Foreign Language [2015, 49]:"></a>Grammar as a Foreign Language <a href="http://de.arxiv.org/pdf/1412.7449" target="_blank" rel="noopener">[2015, 49]</a>:</h5><p>文本分析领域，同样是用LSTM+S2S，输入文本输出语法节点，使用深度优先遍历将语法树变换为序列。Attention部分使用\(e _{ij} = v ^T tanh(W _h ^T h _i + W _s ^T s _t)\)，其中v与W均为学习参数。</p>
<h5 id="BiDAF：Bidirectional-attention-flow-for-machine-comprehension-2016-51"><a href="#BiDAF：Bidirectional-attention-flow-for-machine-comprehension-2016-51" class="headerlink" title="BiDAF：Bidirectional attention flow for machine comprehension[2016, 51]:"></a>BiDAF：Bidirectional attention flow for machine comprehension<a href="arxiv.org/pdf/1611.01603">[2016, 51]</a>:</h5><p>面向问答场景，输入是文字段落+相关问题，输出是问题的答案。BiDAF设计了分层的多阶段体系结构，其结构如下图。<br><img src="/.io//BiDAF.png" alt="bubble"></p>
<p>对于输入文字段落与相关问题，BiDAF设计了word embedding层(GLOVE)和Character embedding层(Char-CNN)，然后输入到Contextual结合上下文进行进一步embedding聚合，从而得到ContextEmbedding结果和QueryEmbedding结果。<br>在得到Embedding结果后，与RNNSearch的attention不同，BiDAF使用了双向attention，根据Context对Query进行加权求和得到Query-Context表示(C2Q)（哪个Query词汇对当前Context更相关），根据Query对Context进行加权求和得到Context-Query(Q2C)表示（哪个Context词汇对当前Query更相关）。其具体attention过程为：</p>
<p>$$<br>S _{ij} = w ^T[h;u;h \cdot u] \\<br>a _{t} = softmax(S _{t:}) \\<br>\hat U _{:t} = \sum _j a _{tj}U _{:j} \\<br>b = softmax(max _{col}S ) \\<br>\hat h = \sum _t b _t H _{:t}<br>$$</p>
<p>其中S可以看作Query与Context的相关矩阵。<br>\(\hat U \in R ^{2d \times T}\)，即对每一个Context都对应一个Query Embedding空间的结果（理解为，当前context表征了问题哪一方面）。<br>对于Context-Query，因为对于Query的每一个词找到相应的Context似乎意义不大，切可能因为Context过多导致冗余信息累加后Attention趋于平均，因此我们使用了max函数，找到context与query中词最大的相似度作为分量。这一步骤个人理解为，query整体应该反应的context某一个方面，而不是query每一个词反映了context某个方面。因此没有像Query-Context一样形成矩阵。由于Context-Query是单个向量，为了拉齐，我们duplicate该向量T次，构成\(\hat H \in R ^{2d \times T}\)<br>我们汇总所有表示：<br>$$<br>G _{:t}=[H _{:t};\hat U _{:t} ;\hat U _{:t} \cdot H _{:t};\hat H _{:t} \cdot H _{:t}] \in R ^{8d\times T}<br>$$<br>G为输入到下一层的数据，其由context embedding结果，C2Q结果以及context embedding与C2Q、Q2C特征交叉得到。<br>之后的ModelingLayer 采用了普通的双向LSTM，而OutputLayer则根据不同任务目标设定了不同模块。</p>
<h5 id="BiDAF：Bidirectional-attention-flow-for-machine-comprehension-2016-52"><a href="#BiDAF：Bidirectional-attention-flow-for-machine-comprehension-2016-52" class="headerlink" title="BiDAF：Bidirectional attention flow for machine comprehension[2016, 52]:"></a>BiDAF：Bidirectional attention flow for machine comprehension<a href="arxiv.org/pdf/1707.00896">[2016, 52]</a>:</h5><p>HAN方法针对文本分类问题，其采用了双层GRU，一层编码Word，一层编码sentence。在word层和sentence层输出分别加入attention，希望使用attention使分类问题更聚焦与关键词和关键句。如下图：<br><img src="/.io//HAN.png" alt="bubble"><br>其Attention部分的特殊之处在于，并没有对两个状态进行相似度计算，而是引入了一个参数\(u _w\)与状态计算相似度，即query向量是未知的，希望优化能计算出这个参数。其Attention部分如下：<br>$$<br>e _{ij} = u _w ^T tanh(Wh _{ij} + b)<br>$$</p>
<h5 id="Dynamic-coattention-networks-for-question-answering-2016-53"><a href="#Dynamic-coattention-networks-for-question-answering-2016-53" class="headerlink" title="Dynamic coattention networks for question answering[2016, 53]:"></a>Dynamic coattention networks for question answering<a href="arxiv.org/pdf/1611.01604">[2016, 53]</a>:</h5><p>目标也为文档问答，输入为文档与问题，输出为文档中文本起止位置。使用该方法使用attention来对文档和问题embedding进行交叉，经过交叉合并得到Embedding向量，其思路与BiDAF类似，也是通过不同方向的attention对D和Q进行交叉。</p>
<p><a name="Pointer"> </a></p>
<h5 id="Ptr-Net：Pointer-Networks-NIPS2015-54"><a href="#Ptr-Net：Pointer-Networks-NIPS2015-54" class="headerlink" title="Ptr-Net：Pointer Networks[NIPS2015, 54]:"></a>Ptr-Net：Pointer Networks<a href="https://arxiv.org/abs/1506.03134v1" target="_blank" rel="noopener">[NIPS2015, 54]</a>:</h5><p>解决了输出序列词汇表和输入序列有关的问题（如从输入序列选词，这也是Pointer一词的由来），即输出词汇表可变问题。Ptr-Net主要是在decode输出端，将decode向量与encode向量结合，引入attention机制，利用attention权重从输入中挑选输出。</p>
<p><a name="Pointer+"> </a></p>
<h5 id="Ptr-Net：Pointer-Networks-ACL2017-55"><a href="#Ptr-Net：Pointer-Networks-ACL2017-55" class="headerlink" title="Ptr-Net：Pointer Networks[ACL2017, 55]:"></a>Ptr-Net：Pointer Networks<a href="arxiv.org/pdf/1704.04368">[ACL2017, 55]</a>:</h5><p>在生成文本摘要时，有两种方式extractive（从原文获得）和abstractive（新生成词汇）。利用Ptr-Net，我们可以得到extractive方式，本文则通过融合Ptr-Net和S2S方法，是摘要生成时词汇即可以从原文生成也可以使用新词汇。通过学习得到一个概率Pgen，来判断是使用S2S生成新词还是使用Ptr-Net从现有词汇中选择，同时在计算Pgen时，还加入了计算文本重复度的分量，从而降低重复。</p>
<h5 id="Fusionnet-Fusing-via-fully-aware-attention-with-application-to-machine-comprehension-2017-56"><a href="#Fusionnet-Fusing-via-fully-aware-attention-with-application-to-machine-comprehension-2017-56" class="headerlink" title="Fusionnet: Fusing via fully-aware attention with application to machine comprehension[2017, 56]:"></a>Fusionnet: Fusing via fully-aware attention with application to machine comprehension<a href="https://www.microsoft.com/en-us/research/uploads/prod/2018/02/b0b85ca188da6371c069c82afa71ec9e928da098.pdf" target="_blank" rel="noopener">[2017, 56]</a>:</h5><p>目标仍为文档问答，作者将之前的问答问题框架总结成如下架构：<br><img src="/.io//FusionNet.png" alt="bubble"><br>其中：<br>(1)Word-level fusion：即在word层面对文档D和问题Q进行融合，但是当单词在不同场景下具有不同语义时，无法正确学到。<br>(2)High-level fusion：是对D和Q进行语义抽取之后再进行融合，容易失去细节<br>(2’)High-level fusion (Alternative)：将High-level的Q信息与Word-level的D进行融合<br>(3)Self-boosted fusion？：在model阶段，将D抽象出的context与自身融合，从而获得上下文信息，这一阶段往往在融合了Q之后。<br>(3’)Self-boosted fusion (Alternative)？：将文档信息融合进Q中（根据文档内容提炼Q的重点？作者举例了BiDAF）<br>因为先前工作，大多只使用了上述框架2-3点，因此作者提出了一个复杂的融合模型，并设计填充了上述框架的每一步。</p>
<p>对于输入D，FusionNet使用了五个embedding特征：<br>300-dim word-level embedding<a href="https://zhuanlan.zhihu.com/p/42073620" target="_blank" rel="noopener">Glove</a>（LSA全局共现矩阵做svd，word2vec基于滑动窗口可以encodeing上下文，Glove结合两者，生成共现矩阵时使用滑动窗口只计算局部共现）<br>600-dim contextualized vector<a href="https://arxiv.org/pdf/1708.00107.pdf" target="_blank" rel="noopener">CoVe</a>(Glove输入LSTM Encoder-Decoder得到Encoder表示)<br>12-dim POS embedding<br>8-dim NER embedding<br>1-dim 归一化的词频<br>对于输入Q，使用300-dim Glove与600-dim CoVe。<br>具体流程如下：<br><img src="/.io//FusionNet2.png" alt="bubble"></p>
<p>其中Word-level fusion：\(\hat w ^C _i = [w ^C _i ; em _i; \hat g ^C _i ]\)，其中em为判断C是否在Q中存在向量，g为300-dim Glove，引入attention：<br>$$<br>\hat g ^C _i =\sum _j \alpha _{ij}g􏰄 ^Q _j, α _{ij}∝exp(S(g ^C _i ,g ^Q _j)), S(x,y)=ReLU(Wx) ^TReLU(Wy)<br>$$<br>其余过程已在图中补完。FusionNet总结了过去问答过程对文档和问题处理融合的流程，并设计了一套复杂的流程，将使用了流程中全部融合思路，以确保信息最大化保持与抽象。同时提出了一个history-of-word概念，即将各个level的context拼接到了一起，在最上层形成一个超大向量。</p>
<h5 id="Effective-Approaches-to-Attention-based-Neural-Machine-Translation-2015-57"><a href="#Effective-Approaches-to-Attention-based-Neural-Machine-Translation-2015-57" class="headerlink" title="Effective Approaches to Attention-based Neural Machine Translation[2015, 57]:"></a>Effective Approaches to Attention-based Neural Machine Translation<a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">[2015, 57]</a>:</h5><p>RNNSearch是在S2S中间插入attention，使decode部分能融合更多encode的上下文。而本文则是在decode输出时融入attention，作者提出两种方式，一种全局方法，其中所有源词都被关注，一个局部方法，其中一次只考虑源词的一个子集。使用局部方法，主要是因为在机器翻译时，翻译的语序往往和被翻译的文本某一块局部对应。局部翻译的方法框架因此如下图：</p>
<p><img src="/.io//Rocktaschel_et_al_%5B57%5D.png" alt="bubble"></p>
<p>其中pt是根据当前ht学习出的位置，然后pt为中心基于正态分布，对attention的权重进行调整。由于被翻译过的词不用再翻译，在上图基础上，作者还将上一步attention的输出接入decode的当前步中，以避免重复。该模型<a href="#BLEU">BLEU</a>好于RNNSearch6.5个点。</p>
<h3 id="2-1-2-Attentional-interfaces-in-Computer-Vision"><a href="#2-1-2-Attentional-interfaces-in-Computer-Vision" class="headerlink" title="2.1.2 Attentional interfaces in Computer Vision"></a>2.1.2 Attentional interfaces in Computer Vision</h3><p>事实上，人的视觉在识别图像事物时，为了区分图像，人的眼睛往往会聚焦到事物的某一些特征，因此CV领域使用attention机制也是非常reasonable的。对于CNNs来说，首先CNNs的参数会随着图片大小的增长而增长。其次，如果识别目标的两个特征距离很远，CNNs需要很高的层词才能捕捉到长距离依赖。而实际上，人眼识别场景时，其实往往具有扫过或余光这样的，长距离快速定位的能力，我们不需要理解场景的每一个细节，再建立联系。</p>
<h5 id="RAM：Recurrent-models-of-visual-attention-2014-46"><a href="#RAM：Recurrent-models-of-visual-attention-2014-46" class="headerlink" title="RAM：Recurrent models of visual attention[2014, 46]:"></a>RAM：Recurrent models of visual attention<a href="https://arxiv.org/abs/1406.6247" target="_blank" rel="noopener">[2014, 46]</a>:</h5><p>RAM使用了RNN，并借鉴了人类Glimpse行为，其核心思想是，根据当前状态计算出下一块聚焦图片块的位置。作者使用了基于强化学习方法的注意力机制，使用收益函数来进行模型训练。其过程如下图：</p>
<p><img src="/.io//RAM.png" alt="bubble"><br>图A说明RAM是从原始图片中根据位置l来得到不同尺寸大小的图片块。图B是得到图C中RNN输入的过程，其将位置信息与根据位置得到的图片块进行融合。对于C中RNN，我们根据状态得到两个输出，输出action与位置信息l。其中位置信息作为下一轮RNN的输入，而输出action根据各自不同的任务，可以take不同的action，如分类任务可以作为分类softmax的输入。RAM与之前加权平均的attention不同，他通过位置生成的方式从而形成一种类似hard attention的过程。</p>
<h5 id="STN：Spatial-Transformer-Networks-2015-59"><a href="#STN：Spatial-Transformer-Networks-2015-59" class="headerlink" title="STN：Spatial Transformer Networks[2015, 59]:"></a>STN：Spatial Transformer Networks<a href="https://arxiv.org/abs/1506.02025" target="_blank" rel="noopener">[2015, 59]</a>:</h5><p>传统CNN网络的池化层具有小尺度平移不变性，而对于实际图片，我们希望学习对于图片具有空间变换不变性（将平移、旋转、缩放及裁剪不变性）。为了使网络对放射变换后的图片仍能很好的学习，STN提出可以输入的图片或者特征进行仿射变换，来使其能变换成容易学习的角度或样子。因为每个图片可能需要的变换方式不同，因此，作者希望通过对输入图片进行学习，从而学到放射变换矩阵A，在对输入进行变换（self-attention），变换后，由于无法和原输入网格对齐，因此STN采用双线性插值得到新块。在学习A的过程可以使用一个子网络或其他方法，如全链接或卷机网络。由于整个过程是一个模块化的，因此可以插入神经网络的任何部分（如CNN的某一层间）。</p>
<h5 id="Draw-A-recurrent-neural-network-for-image-generation-2015-36"><a href="#Draw-A-recurrent-neural-network-for-image-generation-2015-36" class="headerlink" title="Draw: A recurrent neural network for image generation[2015, 36]:"></a>Draw: A recurrent neural network for image generation<a href="http://proceedings.mlr.press/v37/gregor15.pdf" target="_blank" rel="noopener">[2015, 36]</a>:</h5><p>DRAW主要用于图像生成，在RAM基础上进行了改进。整个生成模型，由RNN的encoder和decoder组成。DRAW采用了迭代patch图像的方法，其思路类似油画绘画时一层一层涂色，每次完成一个局部的一部分，最终成画。其结构如下：<br><img src="/.io//DRAW.png" alt="DRAW"><br>图中左边是普通的auto-encoder，右边是DRAW。对于整个网络Read部分使用了attention机制，其输入为图像x、上一步decode的状态h^dec，上一步生成的图像与输入差（error image hat_x）。这里使用了高斯滤波，使用h^dec计算出滤波中心、方差等参数，然后使用该滤波器对输入x和图像差hat_x上进行滤波得到结果输入Encoder。Encoder对图像信息进行压缩，输出经过采样得到decoder输入（这里没有很理解，采样后如何进行反向传导??）。decoder输出进入write，这里write使用和reader相同的方法得到滤波器参数（滤波器应该是和reader对称的），然后将图像还原并且patch到生成图像中。</p>
<h2 id="2-2-Multimodality"><a href="#2-2-Multimodality" class="headerlink" title="2.2 Multimodality"></a>2.2 Multimodality</h2><p>过去Attention Inferface仅仅被适用于单个模型或单一任务，如翻译问题，图像识别问题。而如今，attention<br>Inferface逐渐作为一个链接多模型的工具而被广泛使用。</p>
<h3 id="2-2-1-图像到文本"><a href="#2-2-1-图像到文本" class="headerlink" title="2.2.1 图像到文本"></a>2.2.1 图像到文本</h3><h5 id="Show-attend-and-tell-Neural-image-caption-generation-with-visual-attention-ICML2015-45"><a href="#Show-attend-and-tell-Neural-image-caption-generation-with-visual-attention-ICML2015-45" class="headerlink" title="Show, attend and tell: Neural image caption generation with visual attention[ICML2015, 45]:"></a>Show, attend and tell: Neural image caption generation with visual attention<a href="arxiv.org/pdf/1502.03044">[ICML2015, 45]</a>:</h5><p>作为第一个使用attention 在多模型问题的文章，它解决的主要问题是从图片中生成文字标题（Image Captioning），其主要融合了CNN与LSTM。实际上[45]是将RNNSearch encoding层替换成了CNN，用CNN来抽象图片特征，并将特征输入Decode网络，这里CNN使用了浅层，即使用较为局部信息。Decode部分作者仍使用LSTM，对于每一步输入[45]使用Attention机制，利用上一步RNN状态计算出图像特征attention权重（使用MLP计算权重），然后加权图像特征， 作为RNN输入。对于attention部分，作者提出了两种方式，一种是加权方式的soft attention，另一种是基于统计的hard attention，其中hard attention使用One-hot，在attention步只从图片特征中只取其中一个，但因为其过程基于概率，优化时采用极大似然估计，因此加入网络后不太容易使用通常的方式进行优化。  </p>
<p>基于[45]的思路，有很多加以改进，或应用于不同领域上的研究，例如：<br>视频到文本数据<br>**Describing Videos by Exploiting Temporal Structure[ICCV2015, 62]**：简单的将<a href="#RNNSearch">RNNSearch[45]</a>的思路用于视频，对视频每一帧用CNN抽取特征，使用soft attention+LSTM学习文本摘要，attention部分使用\(e _{ij} = v ^T tanh(W _h ^T h _i + W _s ^T s _t)\)。<br>**Hierarchical Attention-based Multimodal Fusion for video captioning<a href="https://www.sci-hub.ren/10.1016/j.neucom.2018.07.029" target="_blank" rel="noopener">[2018, 63]</a>**：使用了<a href="#RNNSearch">RNNSearch[45]</a>的思想，作者将视频分为视频特征（时序图像特征、运动特征等）、音频特征、情感特征等不同模块，用各自模块的网络分开训练表示特征。其中，对于使用RNN的模型，模型输出下部分网络前和[RNNSearch[45]]一样一律加入attention融合过去状态，对于不同模块间特征融合，也使用attention，加权个部分特征。最后汇总特征到一个向量中，并在encoder层前加入attention，最终训练出视频的描述。<br>**Memory-augmented attention modelling for videos [2016, 64]**：针对视频，Encoder部分不在使用CNN，而是使用LSTM，CNN抽取的特征作为Encoder LSTM的输入，同时encoder和decoder之间采用attention链接，attention部分使用\(e _{ij} = v ^T tanh(W _h ^T h _i + W _s ^T s _t)\)。<br>图像到文本数据<br>**A diagnostic report generator from ct volumes on liver tumor with semi-supervised attention mechanism[65]**：FCN对CT中肝脏和肿瘤进行图像分割，输入CNN抽取特征，LSTM学习语义CNN之后过程与与<a href="#RNNSearch">RNNSearch[45]</a>保持一致。<br>**Adaptive feature abstraction for translating video to text[AAAI2018 , 66]**：对视频使用3D的CNN作为encoder其余与<a href="#RNNSearch">RNNSearch[45]</a>保持一致。</p>
<p><a name="Q67"> </a></p>
<h5 id="monocular-RGB-D-images-Global-context-aware-attention-lstm-networks-for-3d-action-recognition-CVPR2017-67"><a href="#monocular-RGB-D-images-Global-context-aware-attention-lstm-networks-for-3d-action-recognition-CVPR2017-67" class="headerlink" title="monocular/RGB-D images Global context-aware attention lstm networks for 3d action recognition[CVPR2017, 67]:"></a>monocular/RGB-D images Global context-aware attention lstm networks for 3d action recognition<a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_Global_Context-Aware_Attention_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[CVPR2017, 67]</a>:</h5><p>运动捕捉主要是捕捉关节点，这里作者将关节点根据连接性构成序列，每一个时间点下每一个关节点对应一个LSTM，该方法是对ST-LSTM进行了改进，其结构如下：<br><img src="/.io//GCA-LSTM.png" alt="bubble"><br>与过去的LSTM不同，这里使用了一种参数共享的迭代方式的多层2D LSTM，其中一层LSTM有J*T个LSTM Cell。若迭代N轮则相当于N层，每层J*T个cell，且不同迭代层同位置参数相同的深层(N层)2D LSTM。<br>对于每一个LSTM节点，输入为：当前t下第j个关节x _{t,j}，上一时间t-1下j关节状态h_{t-1,j}，当前t下第j-1个关节h _{t,j-1}。在对每一步LSTM进行求解时，引入全局迭代的2D attention机制。对时间和空间上进行全局attention：<br>$$<br>e _{j,t} ^{(n)} = W _{e _1}(tanh(W _{e _2}(h _{j,t};F ^{(n-1)})))\\<br>r _{j,t} ^{(n)} = \frac {exp(e _{i,j} ^{(n)})}{\sum _p \sum _q exp(e _{p,q} ^{(n)})}<br>$$<br>其中，F为上一层迭代轮的全局上下文信息，h为上一层迭代轮同位置LSTM输出，attention计算相当于根据上一轮学到的全局信息+上一轮该位置的信息一起判断该位置的重要性权重。对于求得的attention权重r，将其融入到LSTM中，对于输入乘r，对于状态信息乘(1-r)。即若当前部分不重要，则使用更多历史或周围的状态信息。<br>对于每一轮迭代（或说对于每一层的Global Context初的attention）应具有不同的参数。其迭代公式如下：<br>$$<br>F ^{(n)} =ReLu(W _{F}(h ^{(n-1)} _{J,T};F ^{(n-1)}))\\<br>$$<br>其中h为上一轮最后一个cell输出状态，F仍为上一层迭代轮的全局上下文信息。<br>当全部层完成学习，我们得到了最终的F，若我们执行的任务为分类任务，则：<br>$$<br>\hat y =  softmat(W _c(F ^{(n)}))<br>$$<br>这里可以理解为，通过N轮迭代后，F可以encoding出全局的信息。<br>原文里，对于这种深层RNN结构，作者没有描述如何训练，但其后续文章（Skeleton-Based Human Action Recognition With Global Context-Aware Attention LSTM Networks, TIP2018）中有详细描述，这里附加一张后续文章中对训练部分对描述图，同时通过该图，也方便大家对整个网络结构进行理解：<br><img src="/.io//GCA-LSTM_train.png" alt="bubble"><br>可以看到作者使用了贪心逐步调优的方式，对整个网络进行优化。<br>对于整个文章，作者没有使用传统的LSTM对信息进行抽象，而是使用了attention的全局信息，同时这种参数共享的多层LSTM+迭代式更新的Attention也有一些借鉴意义。</p>
<h5 id="Attention-in-convolutional-lstm-for-gesture-recognition-NIPS2018-68"><a href="#Attention-in-convolutional-lstm-for-gesture-recognition-NIPS2018-68" class="headerlink" title="Attention in convolutional lstm for gesture recognition[NIPS2018, 68]:"></a>Attention in convolutional lstm for gesture recognition<a href="https://core.ac.uk/download/pdf/224778032.pdf" target="_blank" rel="noopener">[NIPS2018, 68]</a>:</h5><p>作者对ConvLSTM进行了优化由于ConvLSTM中使用了大量卷积操作，因此计算量较高。作者提出了四种不同的结构，来对ConvLSTM进行优化：<br><img src="/.io//AConvLSTM.png" alt="bubble"><br>其中，a）将所有门中卷积结构替换成了Pooling+全链接，因此每一个feature对应门为一维数值（使用卷积为K*K矩阵），b）对a进行改进，对输入加入attention（attention中X与H参与计算），这时改变后的输入对所有门和计算都产生影响。c）对a进行改进，对输入门加入attention（wtanh(wh+wx)型式其中全部为卷积，attention中X与H(t-1)参与计算，仍使用卷积），输入门输出不再使用sigmoid转而使用attention权重，这里attention计算权重时分母不是sum而是max（防止用sum平均之后门值过小）。d）对c进行改进，不重建输入门，而对输出门加入attention。<br>作者试验结果表明，b效果低于ConvLSTM，acd三个效果好于ConvLSTM，且a效果好于cd。说明门中使用卷积以及对输入、输入门、输出门嵌入注意力机制并不能促进特征融合。（这里作者只验证了wtanh(wh+wx)一种方式的注意力机制，且因为注意力机制是建立在卷积上的，本身卷积对融合就没有帮助(按照作者的论证)，因此只能论证注意力机制对ConvLSTM这样使用卷积的网络没有帮助）</p>
<h5 id="Attention-in-convolutional-lstm-for-gesture-recognition-ECCV2018-69"><a href="#Attention-in-convolutional-lstm-for-gesture-recognition-ECCV2018-69" class="headerlink" title="Attention in convolutional lstm for gesture recognition[ECCV2018, 69]:"></a>Attention in convolutional lstm for gesture recognition<a href="https://core.ac.uk/download/pdf/224778032.pdf" target="_blank" rel="noopener">[ECCV2018, 69]</a>:</h5><p>作者使用sigmoid(wh+wx+b)作为attention参数，根据h(t-1)与x对输入进行了重构（sigmoid(~)·x），并用这种增强方式，对普通RNN、LSTM、GRU，三种不同的RNN进行了测试，结果表明这种对输入使用attention重构的方法，效果比没有重构要好（疑惑，这不就是相当于在RNN外给输入加了个门吗？）。</p>
<p>**remote sensing data<br>Description generation for remote sensing images using attribute attention mechanism[2019, 70]**：Image Captioning，与<a href="#RNNSearch">RNNSearch[45]</a>近乎一致的结构，其中CNN浅层与深层的特征同时被使用，attention部分由MLP学出。</p>
<h5 id="Hyperspectral-images-classification-based-on-dense-convolutional-networks-with-spectral-wise-attention-mechanism-2019-71"><a href="#Hyperspectral-images-classification-based-on-dense-convolutional-networks-with-spectral-wise-attention-mechanism-2019-71" class="headerlink" title="Hyperspectral images classification based on dense convolutional networks with spectral-wise attention mechanism[2019, 71]:"></a>Hyperspectral images classification based on dense convolutional networks with spectral-wise attention mechanism<a href="https://www.mdpi.com/2072-4292/11/2/159/pdf" target="_blank" rel="noopener">[2019, 71]</a>:</h5><p>Hyperspectral images（HSI）高光谱图像，是一种三维图像，相比于普通图片，其通道数极多，为了更好的对图像中各部分进行聚类，作者结合Attention、空洞卷积、残差网络，对传统CNN进行改进。首先，由于对图像内部各个像素进行聚类，是一种稠密预测，因此作者使用了Dilated Convolution代替传统CNN卷积与Pooling，保证稠密。同时由于CNN由浅到深，逐层抽象，而进行聚类时，作者希望能同时关注到细节与上层抽象表达，因此使用残差网络思路，每一层输出连接之后的所有层。因为引入残差网络+HSI数据本身通道就很多，残差网络融合时，作者希望引入attention，对残差融合进行加权优化，这里使用SE Block 计算Attention权重。最终输出使用softmax生成map图对图像内部进行分类。</p>
<h5 id="Scene-classification-with-recurrent-attention-of-vhr-remote-sensing-images-2018-72"><a href="#Scene-classification-with-recurrent-attention-of-vhr-remote-sensing-images-2018-72" class="headerlink" title="Scene classification with recurrent attention of vhr remote sensing images[2018, 72]:"></a>Scene classification with recurrent attention of vhr remote sensing images<a href="https://sci-hub.se//https://ieeexplore.ieee.org/abstract/document/8454883/" target="_blank" rel="noopener">[2018, 72]</a>:</h5><p>5层CNN，用最后一层作为图片抽象特征。用3层LSTM尝试学习图像特征表达，所有特征反复输入到每一轮LSTM，用最后一层LSTM输出计算softmax作为attention参数，加权输入，进行下一轮迭代。整个过程可以理解为使用LSTM机制学习对特征的attention，类似人类观察图片，不断聚焦不同特征，最终作出判断，同时人在关注不同特征时，之前的观察结果也会给予一定参考，整个过程与LSTM机制类似，因此这里使用LSTM是恰当的，作者实验结果1层LSTM和3层准确率相近，3层略好，更多的层反而降低了准确率，同时RNN迭代轮数来看从1到30轮准确率逐轮递增。</p>
<p>**Spectral-spatial attention networks for hyperspectral image classification [2019, 73]**：对高光谱数据，作者将特征分为两部分，一个频域一个空域。频域是对每个像素位置不同频段看作一个序列，利用Bi-RNN输出计算Attention权重，利用权重重构频域得到特征。空域使用Attention先对小像素片不同频段进行attention叠加，然后2层CNN抽象空域特征。频域特征和空域特征共同输入到MLP中进行融合学习分类</p>
<h5 id="audio-video-Attention-based-multimodal-fusion-for-video-description-ICCV2017-74"><a href="#audio-video-Attention-based-multimodal-fusion-for-video-description-ICCV2017-74" class="headerlink" title="audio-video Attention-based multimodal fusion for video description[ICCV2017, 74]:"></a>audio-video Attention-based multimodal fusion for video description<a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Hori_Attention-Based_Multimodal_Fusion_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[ICCV2017, 74]</a>:</h5><p>和<a href="#RNNSearch">RNNSearch[45]</a>在输入前attention融合不同，本文在decoder的输出端进行融合。首先对每一帧图像用CNN抽象出的特征，利用RNN上一轮状态计算图像特征attention，对全部帧加权求和，attention的结果和decoder的LSTM输出进行融合(状态和图像结果各自加权求和+softmax)，共同预测最终结果，对于decoder输入为上一轮状态和上一轮输出（即上一轮和图像融合后的输出）。  其过程如下：<br><img src="/.io//Hori_Attention-Based_Multimodal_Fusion_ICCV_2017.png" alt="bubble"></p>
<p>**An attention guided factorized bilinear pooling for audio-video emotion recognition[IJCNN2019, 75]**：对音频视频分别CNN，然后特征使用矩阵投影到同一空间进行相乘融合。融合后MLP学习情感分类。</p>
<p>**diverse sensors<br>Memory fusion network for multi-view sequential learning <a href>[AAAI2018, 76]</a>**：<br>针对多视图或多通道含有时序概念的数据，对每个通道的数据单独训练一条LSTM网络。<br>在每一轮LSTM迭代时，将相邻时间轮状态输入到旁路网络进行融合，旁路网络首先使用attention融合两个相邻时间点的数据得到\(c _t ^{[t-1,t]}\)（实际上是使用MLP训练attention参数，然后参数叠加到各通道进行通道调整）。<br>然后将融合后的结果输入到一个类似门控RNN的网络里进行再次融合。该网络将融合特征输入到三个神经网络里，其中一个根据融合特征学习特征状态u，一个学习历史状态门r1，一个学习当前特征门r2。最后将当前特征门r2与学好的特征状态u相乘，上一轮状态与状态门r1相乘，二者相加得到新的状态（即类似门控RNN，融合了多通道）。最终输出特征分为各自通道LSTM训练出的单通道特征与融合RNN学习出的融合特征两部分。<br>整个文章思路为LSTM训练各通道表达，同时设计旁路网络融合各通道各自状态，得到融合特征。其架构如下：<br><img src="/.io//AAAI2018_77.png" alt="bubble"></p>
<h5 id="Multiattention-recurrent-network-for-human-communication-comprehension-AAAI2018-77"><a href="#Multiattention-recurrent-network-for-human-communication-comprehension-AAAI2018-77" class="headerlink" title="Multiattention recurrent network for human communication comprehension[AAAI2018, 77]:"></a>Multiattention recurrent network for human communication comprehension<a href="https://arxiv.org/pdf/1802.00923.pdf" target="_blank" rel="noopener">[AAAI2018, 77]</a>:</h5><p>作者对LSTM的结构进行了改进，首先每个通道拥有各自的LSTM，对于各个通道的状态输入到Multi-attention Block中得到融合的状态z _t，该融合状态输入到各自通道LSTM中参与迭代(即各自LSTM输入包含各自输入，各自状态，融合状态三个部分，各自部分有各自参数矩阵)。Multi-attention Block部分作者以各自LSTM结果h作为输入，使用DNN学习出K*M’维attention融合系数（M’为M个通道各自特征数之和），然后乘以各自的h得到K*M’维特征矩阵，然后对各个通道的特征矩阵进行各自降维，得到各自降维聚合特征后再输入DNN，还原回M’维特征向量作为融合后的输出。（这里有一些疑问，首先使用DNN对特征进行融合输出扩展了K倍的特征，为什么这样做、好处是什么、不同K效果如何作者没有给出解释，其次对于降维、和两次DNN具体过程作者没有详细阐明。因此整个MAB部分令人迷惑，为什么要先DNN扩张K倍，再降维，再DNN还原回原大小，为什么不直接使用DNN作为attention过程，是本文令人疑惑的地方。）</p>
<h5 id="Relational-recurrent-neural-networks-NIPS2018-78-："><a href="#Relational-recurrent-neural-networks-NIPS2018-78-：" class="headerlink" title="Relational recurrent neural networks [NIPS2018, 78]："></a>Relational recurrent neural networks <a href="https://proceedings.neurips.cc/paper/2018/file/e2eabaf96372e20a9e3d4b5f83723a61-Paper.pdf" target="_blank" rel="noopener">[NIPS2018, 78]</a>：</h5><p>传统的LSTM状态为一维向量，表达历史信息较少，本文通过引入Self-attention机制，将状态转化为记忆矩阵，从而能容纳更多的历史信息。其具体流程如下：</p>
<p><img src="/.io//Relational_recurrent_neural_networks.png" alt="bubble"></p>
<p>其中Self-attention部分，引入了记忆矩阵M，M融合了Query、Key、Value的信息。对于输入x，我们通过线性投影得到：查询\(Q=MW ^q\)，键\(K=MW ^k\)和值 \(V= MW ^v\)。我们有查询结果：<br>$$<br>A(Q,K,V) = softmax(\frac {QK ^T}{\sqrt{d _k}})V<br>$$</p>
<p>其中dk为key向量的维数，用作比例因子。<br>当有了新的输入x，我们需要更新记忆矩阵M：<br>$$<br>M’  = softmax(\frac {Q([M;x]W ^k) ^T}{\sqrt{d _k}})[M;x]W ^v<br>$$<br>通过将该方法嵌入到现有的2D-RNN中，得到本文提出的方法，如下图引入2D-LSTM。</p>
<p><img src="/.io//Relational_recurrent_neural_networks_eq.png" alt="bubble"></p>
<p>其中使用了逐行计算的表示方法，输入门控制的部分通过融合更新的M得到（与传统的x与h加权融合不太相同）</p>
<h5 id="Neural-Multimodal-Belief-Tracker-with-Adaptive-Attention-for-Dialogue-Systems-WWW2019-79-："><a href="#Neural-Multimodal-Belief-Tracker-with-Adaptive-Attention-for-Dialogue-Systems-WWW2019-79-：" class="headerlink" title="Neural Multimodal Belief Tracker with Adaptive Attention for Dialogue Systems [WWW2019, 79]："></a>Neural Multimodal Belief Tracker with Adaptive Attention for Dialogue Systems <a href="https://www.sci-hub.ren/10.1145/3308558.3313598" target="_blank" rel="noopener">[WWW2019, 79]</a>：</h5><p>本文主要设计了一种可以识别图片和文本的AI对话系统，其主要应用在时装零售的场景（例如针对用户提问推荐图片，或者用户对某一张图片提出疑问，基于解答或类似推荐）。作者认为，对于这样的对话系统而言，系统对话图片，用户文本，用户对话图片，对于对话而言是有用的。在这种场景下，对图片特征，作者使用了ResNet50，抽取图片局部特征（零售场景的图片，商品往往占图片中一小部分）。针对文本Encoder，使用了两路LSTM分别encode用户文本与系统文本。对于输出MLP网络的输入，作者只只用了用户图片、系统图片、用户文本信息，使用attention对这三个特征进行融合，其中attention需要参考当前文本encoder的状态信息。其整个过程如下图，可以看书该方法是针对图文混合问答系统特殊构造的网络。<br><img src="/.io//NMBT.png" alt="bubble"></p>
<h5 id="Dual-attention-networks-for-multimodal-reasoning-and-matching-CVPR2017-80-："><a href="#Dual-attention-networks-for-multimodal-reasoning-and-matching-CVPR2017-80-：" class="headerlink" title="Dual attention networks for multimodal reasoning and matching [CVPR2017, 80]："></a>Dual attention networks for multimodal reasoning and matching <a href="http://arxiv.org/pdf/1611.00471" target="_blank" rel="noopener">[CVPR2017, 80]</a>：</h5><p>DAN解决图片文本多模态问题，对于图片，作者仍使用CNN，抽取高层次多个特征片，对于文本使用Bi-RNN抽取出文本特征序列。其融合过程类似<a href="#Q67">[67]</a>，通过对特征多次attention反复迭代，不断转移注意力到新的区域，最终融合每一步结果得到想要的推断。<br>图片部分Attention公式如下，v为图片特征矩阵，m为attention记忆矩阵<br><img src="/.io//DAN_eq_p.png" alt="bubble"><br>文字部分Attention公式如下，u为文字特征矩阵，m为attention记忆矩阵<br><img src="/.io//DAN_eq_t.png" alt="bubble"><br>可以看到文字和图片不同之处在于少了一层投影矩阵P+tanh，其主要目的是将视觉特征投影到文本特征空间。<br>对于attention是如何使用，视频文本又是如何融合的，根据不同应用场景，作者给出了不同的框架。作者设计r-DAN用于视觉文本问答场景，m-DAN用作视觉文本匹配场景，两个不同场景在于，视觉文本问答场景，要求文本和图片共同作用，希望回答能融合文本和图片的信息，而视觉文本匹配问题文本和视觉两个通道相对独立，最终输出是对两个通道进行比较。因此，作者分别设计框架如下：<br><img src="/.io//DAN.png" alt="bubble"><br>从图中可以看出，不同点主要是attention结果是如何传递的，对于r-DAN视觉和文本部分分别维护各自的记忆矩阵m，每一轮对m的更新是直接将输出的v或u叠加在上一轮m中。对于m-DAN，为了融合视觉和文本信息，对于m的更新使用了v和u逐元素各自相乘（element-wise multiplication）叠加到m中，达到混合的目的。同时对于输出r-DAN是将最终的attention记忆矩阵m乘以输出矩阵并Softmax得到结果，而m-DAN则是每一步对u和v内积，最终讲每一轮迭代的内积叠加，得到最终相似度。<br>本文使用了迭代的attention，其整体思路和<a href="#Q67">[67]</a>较为相似，通过将数据扩展到多模态，并根据不同任务设计不同网络结构。但和<a href="#Q67">[67]</a>的融合不同每次attention记忆矩阵进行融合时只采用了直接叠加方式，而<a href="#Q67">[67]</a>的叠加方式更加复杂一些，因此本文attention每次使用记忆矩阵计算attention时，attention区域可能变化不太大。同时使用叠加而不是融合投影的方式，记忆矩阵每次叠加后没有归一化，可能导致记忆矩阵特征值越来越大，当K较大时，对训练可能也会产生影响（个人感觉）。</p>
<h5 id="Bi-directional-Spatial-Semantic-Attention-Networks-for-Image-Text-Matching-TIP2018-81-："><a href="#Bi-directional-Spatial-Semantic-Attention-Networks-for-Image-Text-Matching-TIP2018-81-：" class="headerlink" title="Bi-directional Spatial-Semantic Attention Networks for Image-Text Matching[TIP2018, 81]："></a>Bi-directional Spatial-Semantic Attention Networks for Image-Text Matching<a href="https://www.sci-hub.ren/10.1109/TIP.2018.2882225" target="_blank" rel="noopener">[TIP2018, 81]</a>：</h5><p>本文主要面向文本图片匹配问题，作者设计了一种Bi-directional attention模型。<br>对于文本图片匹配问题，作者认为可以从两个角度考虑，一方面是使用文本，找到图片中可能相关的区域，另一方面是使用图片，找到文本中相关的词汇。对于这两方面，作者分别训练了两路网络，使用CNN抽取图片特征，然后分别根据图片特征对文本特征进行attention聚合，根据文本特征对图片特征进行聚合，从而得到了2路不同的特征表示。最后将2路特征分别输入到各自LSTM+MLP网络计算相似度。<br>其整体过程如下图，在学习过程中，2路相对独立，最终相似度，使用各自输出加权求和。这篇文章的设计思路实际上就是利用多特征通道之间相互attention的不同query与value的组合，得到多路孪生网络。然后分别学习，最终简单对结果进行汇总。<br><img src="/.io//Bi-directional_attention.png" alt="bubble"></p>
<h5 id="Pointing-novel-objects-in-image-captioning-CVPR2019-82-："><a href="#Pointing-novel-objects-in-image-captioning-CVPR2019-82-：" class="headerlink" title="Pointing novel objects in image captioning[CVPR2019, 82]："></a>Pointing novel objects in image captioning<a href="https://arxiv.org/pdf/1904.11251.pdf" target="_blank" rel="noopener">[CVPR2019, 82]</a>：</h5><p>之前的image captioning问题主要是用CNN做特征提取，然后送入LSTM中，这些模型是建立在大量的训练集中的image-caption对数据，只具有in-domain视野，当学习图片和训练集内容差别较大时候，模型效果就会很不好。本文通过使用在其他数据集上训练好的目标检测模型与image-caption训练的LSTM进行融合，从而扩充了词汇库，其过程如下图：</p>
<p><img src="/.io//LSTM-P.png" alt="bubble"><br>整个流程主要部分是Pointing mechine部分，其公式如下：<br>$$<br>Pr ^t _d (w _{t+1}) = \mathbf w ^T _{t+1} M _dh ^t \\<br>Pr ^t _c (w _{t+1}) = \mathbf w ^T _{t+1} M _c ^{(1)}(I _c \cdot \sigma(M _c ^{(2)} h ^t)) \\<br>P _t = \sigma (G _s \mathbf w _t + G _h h ^t + b _p)\\<br>P _r ^t = (1-p _t) \cdot \phi(Pr ^t _d (w _{t+1})) + P _t \cdot (Pr ^t _c (w _{t+1}))<br>$$</p>
<p>可以看到作者使用LSTM状态来计算图片生成词的分布，对于物体识别部分词分布计算同样融合了当前图片生成词汇的状态h，最终融合二者分布的输出词分布使用了一个根据LSTM状态h和上一轮词分布计算出的概率P_t，来对图片生成和物体识别两个分布进行融合。输出的融合思路和<a href="#Pointer+">改进的Pointer Network[55]</a>有相似之处。<br>整体思路创新在于引入已经训练好的物体识别模型来扩充词库，同时设计了一个比较Make sense的融合方法对两个模型进行融合。</p>
<h5 id="Improving-referring-expression-grounding-with-cross-modal-attention-guided-erasing-CVPR2019-84-："><a href="#Improving-referring-expression-grounding-with-cross-modal-attention-guided-erasing-CVPR2019-84-：" class="headerlink" title="Improving referring expression grounding with cross-modal attention-guided erasing[CVPR2019, 84]："></a>Improving referring expression grounding with cross-modal attention-guided erasing<a href="https://arxiv.org/pdf/1903.00839.pdf" target="_blank" rel="noopener">[CVPR2019, 84]</a>：</h5><p>解决Referring Expression问题（or Visual Grounding），作者在<a href="#mattnet">MattNet[462]</a>的基础上进行了改进提出了一种基于attention机制的跨模型擦除输入特征的方法，从而解决了之前算法只关注图像和文本占据主导地位特征，而忽视了可能存在的潜在联系的问题，其做法是通过擦除对于算法而言图像和文本最重要的部分，从而生成不同的训练集，再用这些训练集学习出更多潜在联系。其做法为，对于文本利用算法中的attention参数来做为文本重要性依据，从而选出最高重要性文本进行擦除。对于图片对MattNet方法进行了改进，对于Subject、location、relationship三部分，引入attention机制，不在对一个固定的frame进行判断而是使用attention融合所有proposal frame来进行match，而因为引入了attention，所以frame的重要程度就可以利用其参数评估，因此可以找到最重要的frame对其擦除。</p>
<p>**Pay attention! robustifying a deep visuomotor policy through task-focused visual attention<a href="https://arxiv.org/pdf/1809.10093.pdf" target="_blank" rel="noopener">[CVPR2019, 85]</a>**：<br>解决问题：根据文本指令对视频中的机械臂进行操控（如：讲红盒推到左边）。对于文本，作者使用LSTM进行embedding，对于视频，作者将图像进行分区使用VGG19对每一块进行特征抽取。整个方法流程如下图，为了增加模型抗噪能力，作者使用了VAE-GAN框架，对于真实图片作者使用图片特征+文本的表示进行attention融合，得到每一块权重后，叠加到每一块特征中，从而达到根据文本质量“高亮”相关区域的目的（生成mask）。对于生成虚假图片，作者使用Encoder+Generator生成mask+图片。然后输入Discriminator进行网络调优。这样调优出的Encoder具有一定的抗噪能力。对于指令生成作者对Encoder输出进行三层LSTM生成指令。<br><img src="/.io//CVPR2019_85.png" alt="bubble"></p>
<h2 id="2-3-Attention-augmented-memory"><a href="#2-3-Attention-augmented-memory" class="headerlink" title="2.3 Attention-augmented memory"></a>2.3 Attention-augmented memory</h2><p>传统的RNN因为很难进行长期记忆，为了弥补这一点，很多改进引入了记忆机制，其中比较典型的为Neural Turing Machine (NTM)和Networks (MemNN)。其原理都是使用额外的内存将每一步的特征存储起来，然后定义Query机制，利用当前表示从存储空间查询到相应的特征表示进行融合，这和attention机制有共通之处。其中比较基础的两种为：Neural Turing Machine (NTM) 和Networks (MemNN) 。</p>
<h5 id="Neural-Turing-Machine-NTM-GoogleDeepMind2014-86-："><a href="#Neural-Turing-Machine-NTM-GoogleDeepMind2014-86-：" class="headerlink" title="Neural Turing Machine (NTM)[GoogleDeepMind2014, 86]："></a>Neural Turing Machine (NTM)<a href="https://arxiv.org/pdf/1410.5401.pdf" target="_blank" rel="noopener">[GoogleDeepMind2014, 86]</a>：</h5><p>NTM是端到端可微的，可以被梯度下降算法训练。NTM memory更倾向于short-term的存储（因为查询时仍使用t-1步的一些状态，但因为引入了Mem因此可以保存更长时间的信息。），使用查询与Mem内容相似度来度量，来快速定位Mem位置。其具体框架和过程如下图（更多细节可见：<a href="https://blog.csdn.net/rtygbwwwerr/article/details/50548311" target="_blank" rel="noopener">《Neural Turing Machines-NTM系列》</a>）：<br><img src="/.io//NTM.png" alt="bubble"></p>
<h5 id="Memory-Networks-Facebook2014-38-："><a href="#Memory-Networks-Facebook2014-38-：" class="headerlink" title="Memory Networks[Facebook2014, 38]："></a>Memory Networks<a href="http://arxiv.org/pdf/1410.3916" target="_blank" rel="noopener">[Facebook2014, 38]</a>：</h5><p>MemNN提出了一种通过使用额外的存储空间，将输入表示向量进行存储，评分时取出存储空间中与匹配向量最相似的两个向量，共同学习的模型。其过程大致分为：输入向量表示层I，内存更新层G，内存查询层O，推断层R。文中涉及到的模型主要针对对于QA问题，对于文档，将输入句子X通过I层（如矩阵投影到表示空间）得到表示向量，然后将每一句子进行存储，对于问题同样表示学习映射到表示空间，然后简单使用COSIN距离匹配最相近的两个词，然后输入浅层NN进行推断。整个过程需要训练的参数为表示投影矩阵和推断层。作者使用了类似SVM的margin ranking loss 和随机梯度下降 SGD进行优化。由于这种方法受限于存储空间大小，如果存储空间足够大，则可以对 long-term memory 可以有较好表示，因此往往用于QA task中。<br>基于Memory Networks有很多研究，如下图：</p>
<p><img src="/.io//Memory_Networks.png" alt="bubble"></p>
<h5 id="End-to-end-Memory-Networks-NIPS2015-91-："><a href="#End-to-end-Memory-Networks-NIPS2015-91-：" class="headerlink" title="End-to-end Memory Networks[NIPS2015, 91]："></a>End-to-end Memory Networks<a href="https://arxiv.org/pdf/1503.08895v4.pdf" target="_blank" rel="noopener">[NIPS2015, 91]</a>：</h5><p>在MemNN基础上进行了改进，对于输入文档和输入问题，分别用Embedding矩阵A和B投影到统一表示空间，Mem匹配部分从过去的取Top2改进成类似attention方式，与经典Attention不同的是，这里根据问题表示u学习出的每个记忆slot的权重p没有直接与记忆表示m相乘，而是乘以文档对应的输出表示c（通过投影矩阵C）得到输出o，然后将o与u相加输入浅层神经网络进行分类。其具体过程如下图左边。同时问了增强表示，作者还扩展了多层MemNN，如下图右侧。可见一层E2E MemNet 需要训练的参数为四个矩阵A、B、C、W，整体方法更容易优化。<br><img src="/.io//E2EMemNet.png" alt="bubble"></p>
<h5 id="Video-Object-Segmentation-using-Space-Time-Memory-Networks-ICCV2019-92-："><a href="#Video-Object-Segmentation-using-Space-Time-Memory-Networks-ICCV2019-92-：" class="headerlink" title="Video Object Segmentation using Space-Time Memory Networks[ICCV2019, 92]："></a>Video Object Segmentation using Space-Time Memory Networks<a href="https://arxiv.org/pdf/1904.00607.pdf" target="_blank" rel="noopener">[ICCV2019, 92]</a>：</h5><p>对视频每一帧使用两路CNN分别得到Key（小规模，用于联合memory快速计算attention权重）和Value（detail feature）特征。对于Mem部分，Mem的特征输入为原始图片与Mask，经过两路CNN得到KV（模型可使用图片库做参数预训练，对新的视频第一桢输入可随机生成或用预训练mask模型），对于Query桢使用原始桢做输入，得到KV。通过Query的key联合Mem的Key进行attention加权记忆value得到记忆特征。然后送入Docoder输出mask。具体过程如下：<br><img src="/.io//Space-Time_Memory_Networks.png" alt="bubble"><br>该方法发具有两个encoder，且不共享参数，同时，新学到的桢可以存入Mem，作者每隔N桢存入一次Mem。模型不假设桢间平滑，因此可以使用静态图片先预训练。</p>
<h5 id="Ask-me-anything-Dynamic-memory-networks-for-natural-language-processing（DMN）-ICML2016-93-："><a href="#Ask-me-anything-Dynamic-memory-networks-for-natural-language-processing（DMN）-ICML2016-93-：" class="headerlink" title="Ask me anything: Dynamic memory networks for natural language processing（DMN）[ICML2016, 93]："></a>Ask me anything: Dynamic memory networks for natural language processing（DMN）<a href="http://proceedings.mlr.press/v48/kumar16.pdf" target="_blank" rel="noopener">[ICML2016, 93]</a>：</h5><p>作者在MemNet框架基础上结合RNN设计了一套网络。从输入到Mem部分到输出推断，均使用GRU进行状态学习。对于Imput部分，文档和Query均使用GRU学习表示，Mem部分作者结合MemNet的思想设计了双层GRU来对输入进行二次推断，和传统直接使用Mem信息不同，作者将Mem的信息、Query表示与上一时间RNN状态进行特征融合，然后将输出和上一轮输出加权融合（相当于利用mem、query和信息给GRU增加了一个输出门），注意这episodic memory双层和传统双层RNN不同，输入都为x的表示，第一层前attention query输入为Question表示，第二层则为第一层输出，因此可以达到二次推断的效果（二次推断，如：D. John出门带上了篮球。John去了学校。Q. 篮球在哪。第一轮推断学出了John和篮球关联，第二轮推断学出john去了学校，因此得出篮球在学校）其具体结构如下：<br><img src="/.io//DMN.png" alt="bubble"></p>
<h5 id="Dynamic-memory-networks-for-visual-and-textual-question-answering（DMN-）-ICML2016-88-："><a href="#Dynamic-memory-networks-for-visual-and-textual-question-answering（DMN-）-ICML2016-88-：" class="headerlink" title="Dynamic memory networks for visual and textual question answering（DMN+）[ICML2016, 88]："></a>Dynamic memory networks for visual and textual question answering（DMN+）<a href="http://proceedings.mlr.press/v48/kumar16.pdf" target="_blank" rel="noopener">[ICML2016, 88]</a>：</h5><p>DMN+在DMN基础上加入了改进，受限对于DMN的Input部分，使用了双向GRU，增加反向关联。Attention部分精简了特征交叉，减少了两项交叉项，与上一层状态同时刻状态输入项，对于上一层状态，只选用最后一层，然后在RNN最后使用简单NN将上一层最终输出与本层最终输出融合。同时不在对episodic memory部分GRU增加输出门，而是提出两种方法，直接对历史状态soft attention，或将attention参数直接融合到GRU内部更新门。</p>
<h5 id="Key-Value-Memory-Networks-for-Directly-Reading-Documents-ACL2016-95-："><a href="#Key-Value-Memory-Networks-for-Directly-Reading-Documents-ACL2016-95-：" class="headerlink" title="Key-Value Memory Networks for Directly Reading Documents[ACL2016, 95]："></a>Key-Value Memory Networks for Directly Reading Documents<a href="https://arxiv.org/pdf/1606.03126.pdf" target="_blank" rel="noopener">[ACL2016, 95]</a>：</h5><p>KV-MemNN设计了一种基于KV的循环MemNN。和之前Mem使用全部embedding表征不同，KV-MemNN部分是根据question 在source中选择key至少与x有一个单词相同的N个组成子集(倒排索引)，得到memory subset。得到Mem后作者对question和Mem使用embedding得到query出的Mem的value值0，然后将q与q进行融合生成新的再重新进行KV查询。整个算法流程中，需要训练的为输入空间投影矩阵A，答案投影矩阵B，和一系列q的投影矩阵R。算法对于文本的特征抽取（\(\Phi\)过程）使用了较为简单的方法（如：KB triple或bag-of-words），对特征可能缺乏一定表征能力。</p>
<p><img src="/.io//KV-MemNN.png" alt="bubble"></p>
<h5 id="Memory-based-graph-networks-ICLR2020-87-："><a href="#Memory-based-graph-networks-ICLR2020-87-：" class="headerlink" title="Memory-based graph networks[ICLR2020, 87]："></a>Memory-based graph networks<a href="https://arxiv.org/pdf/2002.09518.pdf" target="_blank" rel="noopener">[ICLR2020, 87]</a>：</h5><p>作者在图神经网络中设计了Mem结构。和之前的Mem结构不同，因为图节点很多，无法使用全部节点信息，这里作者引入了聚类的思想，将聚类中心用作Mem的slot，其聚类中心是通过优化学到的（定义距离，并使用KL散度做损失）。每一层MemLayer，作者使用了多路聚类+多路多头attention+1x1Con来丰富学习到的特征，学到的参数是节点对类别的贡献度，然后乘以输入矩阵，得到每一类别当前特征值（相当于从原来的节点数的N行输入，降到类别数的K行输出），然后输入到1层NN得到降维（对一行表示纬度降维）输出。其算法框架如下：<br><img src="/.io//MemGNN.png" alt="bubble"><br>对于输入，作者设计了两种方法：<br>GMN：使用随机游走做网络关系信息嵌入，使用双层NN最终得到Query的表达<br>MemGNN：使用GAT模型的改进版，利用多路多头attention计算邻居权重，然后加权形成网络结构的嵌入，得到Query表达。<br>个人感觉，作者在GNN中引入聚类+Mem机制，主要是为了达到表示对节点数的降维以及能从同类型的图中学到一种该类型网络通用的规则的目的。</p>
<h5 id="Video-Object-Segmentation-with-Episodic-Graph-Memory-Networks-ECCV2020-89-："><a href="#Video-Object-Segmentation-with-Episodic-Graph-Memory-Networks-ECCV2020-89-：" class="headerlink" title="Video Object Segmentation with Episodic Graph Memory Networks[ECCV2020, 89]："></a>Video Object Segmentation with Episodic Graph Memory Networks<a href="https://arxiv.org/pdf/2007.07020.pdf" target="_blank" rel="noopener">[ECCV2020, 89]</a>：</h5><p>解决的问题主要是视频目标检测任务（VOS，video object segmentation），VOS又可分为O-VOS和Z-VOS（one-shot与zero-shot，即有无第一桢标注）。传统的方法主要基于匹配，而基于匹配的方法要么不容易对第一桢正确目标信息加以利用，要么在场景或目标发生外貌变化时很难进行线上调整。因此，作者引入了Graph Mem的结构，其中Mem Slot以网络形式进行组织，每一个slot以全联接的方式构成网络。其框架如下：<br><img src="/.io//Episodic_Graph_Mem_Net.jpg" alt="bubble"><br>可以看到整个流程需要经过K轮迭代，在训练过程中，我们从整个视频中抽样N桢做Mem Slot初始化，在学习过程中，则使用前N桢进行初始化，然后推断N+1帧。对于之后的帧，每次推断使用第一桢（绝对正确的标注）+ 随机抽取N-2桢之前桢 + 上一桢，然后K步迭代，最终生成推断前表示。</p>
<h5 id="Episodic-camn-Contextual-attention-based-memory-networks-with-iterative-feedback-for-scene-labeling-CVPR2017-90-："><a href="#Episodic-camn-Contextual-attention-based-memory-networks-with-iterative-feedback-for-scene-labeling-CVPR2017-90-：" class="headerlink" title="Episodic camn: Contextual attention-based memory networks with iterative feedback for scene labeling[CVPR2017, 90]："></a>Episodic camn: Contextual attention-based memory networks with iterative feedback for scene labeling<a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Abdulnabi_Episodic_CAMN_Contextual_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[CVPR2017, 90]</a>：</h5><p>对图像进行分块，然后每一块CNN学习出特征，初始化Memslot，对于每一个patch，利用attention融合其他相似块表达，输入rnn迭代生成该patch的新表达，迭代T轮。RNN迭代生成新的patch 表达，并更新Mem slot。</p>
<h2 id="2-4-End-to-end-attention-models"><a href="#2-4-End-to-end-attention-models" class="headerlink" title="2.4 End-to-end attention models"></a>2.4 End-to-end attention models</h2><p>从2017年中，更多的研究开始关注与End-to-end的attention模型，如<a href="#Transformer">Neural Transformer [37]</a>与<a href="#GATs">Graph Attention Networks (GATs)[97]</a>。与之前仅将attention作为工具加入网络的一部分不同，这两种方法都是纯attention框架，这为attention成为深度学习中的重要元素提供了一些理论保证。</p>
<h3 id="2-4-1-Transformer-Based"><a href="#2-4-1-Transformer-Based" class="headerlink" title="2.4.1 Transformer Based"></a>2.4.1 Transformer Based</h3><p><a name="Transformer"> </a><br>首先我们来介绍一下基础的Transformer，Transformer源于Google2017的文章：<strong>Attention is all you need</strong><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">[Google2017, 37]</a>。作者提出了Neural Transformer的模型，Neural Transformer是第一个仅使用attention和全联接神将网络来学习序列数据的模型。其主要解决的也是机器翻译问题，和其他翻译模型一样，Transformer也使用了Encoder-Decoder框架，其具体架构如下：<br><img src="/.io//Transformer.jpg" alt="bubble"><br>我们从下而上从左到右开始分析：<br><strong>Positional Encoding</strong>：对于Transformer模型，由于输入是同时送入网络，所以丢失了序列的位置信息，因此对于模型输入需要选择好的位置信息的表达，并融合进输入。对于好的Positional Encoding首先其值不能发散，如用0、1、2这样的位置信息编码，过大的值的位置会导致学习时产生数值倾斜。同时，像Transformer这样使用位置信息叠加到Embedding结果这种潜入方式，过大的值容易掩埋Embedding信息。其次，好的位置编码最好不能受到文本长度的影响，如除以文本长度均匀映射到[0,1]之间这种做法，对于不同长度的文本，编码后距离相同的两个词，短文本的真正间隔可能要远小于长文本，这样会导致位置编码的相对次序缺失。具体位置编码规则如下：  </p>
<p>$$<br>PE(pos,2i)=sin(\frac {pos}{10000 ^{2i/d _{model}}}) \\<br>PE(pos,2i+1)=cos(\frac {pos}{10000 ^{2i/d _{model}}}) \\<br>$$<br>可以看出，这里作者是将位置信息在频域做了近似分解，而选择这种分解方式的好处是PE(pos+k, 2i)可表示为PE(pos, 2i)的线形表示（根据三角函数公式）：<br>$$<br>PE(pos+k,2i)= PE(pos,2i)\times PE(k,2i+1)+PE(pos,2i+1)\times PE(k,2i)\\<br>PE(pos+k,2i+1)= PE(pos,2i+1)\times PE(k,2i+1)-PE(pos,2i)\times PE(k,2i)\\<br>$$<br>因为PE(k, 2i+1)是常量, 所以就有固定的系数. 也就是某种意义上的相对位置编码.    </p>
<p><strong>Multi-Head Attention</strong>：首先Transformer模型使用了Scaled Dot-Product Attention，即使用内积计算query向量与每一个key的相关性，并将其Softmax做为权重，为了方便表示，我们将query向量复制表示成如下矩阵形式：<br>$$<br>Attention(Q,K,V)=softmax(\frac {QK ^T}{\sqrt{d _k}})V \\<br>Q = [q ,\dots,q ]^T<br>$$<br>注意这里除以d的开方，是因为内积之后方差扩大了d倍，为了弥补这一问题，因此除以方差倍数（个人感觉其原理类似Normalization）<br>其次，Attention为Self-Attention，即Q=K=V。<br>为了学习更多表达，作者引入了Multi-Head Attention，即将Q、K、V投影到不同的潜空间，希望能用这种方式学到数据更多方面的表示：<br>$$<br>MutiHead(Q,K,V)=Concat(head _1,\dots,head _h)W ^O\\<br>head _i= Attention(QW _i ^Q,KW _i ^K,VW _i ^V)<br>$$<br>这里作者设置h=8，各W矩阵将原向量投影到d/h=64维，通过降维投影来表达不同方面，最后将各个表达加权聚合。同时值得注意的是，虽然使用self-attention，QKV实际意义是相同的，但不能使用相同的投影矩阵，尤其是QK的参数矩阵，因为如果使用相同参数矩阵，则attention进行相关性计算时，得到的相关性矩阵会成为对称矩阵，实际上对query和key来说，(词1在query，词2在Key)与(词2在query，词1在Key)这两个重要性得分应该是不同的，因此共享参数矩阵会降低表达。</p>
<p><strong>Add&amp;Norm</strong>：从框架图中也可看出，Transformer使用了残差网络，同时加入了LayerNorm，即输出为LayerNorm(x+sublayer(x))。这里使用LayerNorm主要是为了将中间层输出分布还原回均值为0方差为1的分布，从而降低ICS，防止梯度消失等问题。更多讨论如为什么使用LN而不是BN（<a href="https://www.zhihu.com/question/395811291" target="_blank" rel="noopener">讨论</a>）等，可以参考Transformer与Normalization相关文章。</p>
<p><strong>Feed-Forward</strong>：在进行了Attention操作之后，encoder和decoder中的每一层都包含了一个全连接前向网络，对每个position的向量分别进行相同的操作，包括两个线性变换和一个ReLU激活输出。<br>$$<br>FFN(x)=max(0,xW _1 +b _1)W _2 +b _2<br>$$</p>
<p><strong>Masked Multi-Head Attention</strong>：序列模型在decoder部分加入Masked，更多程度上是一种工程上的保证，主要是为了增加模型并行程度。在训练时，首先训练数据往往是batch输入并以长度相同向量表示，而对于句子由于句长不同，因此短句需要补齐mask padding。同时在训练位置T时，T后的数据属于需要预测的部分，因此不能加入attention，所以用mask将其掩盖。Mask主要是通过乘以Mask矩阵，将多余部分替换成一个很大的负值，从而使其attention权重为0。这样的操作更多是工程上的考虑，当然也可以在前向后向训练中加入逻辑判断，不过这样code会变得复杂。</p>
<p>基于Transformer模型，有很多改进方法，如下图：<br><img src="/.io//Transformer_evo.png" alt="bubble"><br>接下来我们根据年份顺序对各种改进进行简单讲解。</p>
<h4 id="2-4-1-1-Transformer方法的改进"><a href="#2-4-1-1-Transformer方法的改进" class="headerlink" title="2.4.1.1 Transformer方法的改进"></a>2.4.1.1 Transformer方法的改进</h4><h5 id="Weighted-Transformer-：Weighted-transformer-network-for-machine-translation-2017-102"><a href="#Weighted-Transformer-：Weighted-transformer-network-for-machine-translation-2017-102" class="headerlink" title="Weighted Transformer ：Weighted transformer network for machine translation[2017, 102])"></a>Weighted Transformer ：Weighted transformer network for machine translation<a href="https://arxiv.org/pdf/1711.02132.pdf" target="_blank" rel="noopener">[2017, 102]</a>)</h5><p>在基础Transformer上对muti-attention进行了调整对每一个head加入了权重。Base Transformer认为，每个head是平权的，因此直接concat所有header然后使用投影矩阵W^O，投影到表示空间。而Weighted Transformer认为每一个head有各自的表达，有各自的权重，因此每一个head，各自直接投影到表示空间然后乘以head权重k，然后各自输入到FFN进行学习，最终学出的结果乘以权重alpha累加合并到最终表示空间，从而在FFN后融合多个Branch 的head。可以看出Weighted Transformer 实际上是形成了多个Branch，权重k和alpha均是本文新增的要学习的参数，这里k的和与alpha的和均为1。</p>
<p><a name="Star-transformer"> </a></p>
<h5 id="Star-transformer-NAACL2019-103"><a href="#Star-transformer-NAACL2019-103" class="headerlink" title="Star-transformer[NAACL2019, 103])"></a>Star-transformer<a href="https://arxiv.org/pdf/1902.09113.pdf" target="_blank" rel="noopener">[NAACL2019, 103]</a>)</h5><p>对输入长度为N的序列，transformer使用了全联接的attention这样做一方面使得参数量过大（N^2的参数量），训练需要大量样本，另一方面，若序列存在局部相关的先验知识时，对于局部相关的先验几乎没有利用。为此Star-transformer简化了transformer全联接的模型，采用了星型的连接结构，使训练既能捕捉局部相关先验，又能捕获全局或者说长期信息。其具体改进过程如下：<br><img src="/.io//Star-transformer.jpg" alt="bubble"><br>其中外围节点h对应序列每一项，h只与其前后项和全局项s有关，因此在更新h时，只需要对上一轮前后两项、上一轮当前项、上一轮全局项进行attention即可，同时可以看到attention还加入了输入项e，这里可以看作保留了原transformer残差网络的思路。对于全局项s，在更新完全部h后，使用attention融合上一轮s和全部h。可以看到，在h和s的更新时，作者去掉了transformer输出时的两层全联接网络，仅使用ReLU，从而对网络进行了再次简化。<br>Star-transformer最终输出的是s和H，可以将其输入MLP进行分类或作为特征输出到其他任务网络。</p>
<h5 id="Self-Attention-with-Relative-Position-Representations-NAACL2018-174"><a href="#Self-Attention-with-Relative-Position-Representations-NAACL2018-174" class="headerlink" title="Self-Attention with Relative Position Representations[NAACL2018, 174])"></a>Self-Attention with Relative Position Representations<a href="https://arxiv.org/pdf/1803.02155.pdf" target="_blank" rel="noopener">[NAACL2018, 174]</a>)</h5><p>基础transformer对于序列每一项的位置信息仅在输入时使用正弦位置编码来提供，这种方式编码了位置的绝对信息，而本文作者提出一个self-attention的扩展来考虑元素之间的成对关系。对于序列每一项，作者构建了一个全联接图，且令\(a _{ij} ^v,a _{ij} ^k \in R ^{d _a}\)为i和j之间的边（作者将key空间和value空间的边分开进行了表示）。则Attention公式融合边的信息修改为：   </p>
<p>$$<br>clip(x,k) = max(-k,min(k,x)) \\<br>a _{ij} ^K = w _{clip(j-i,k)} ^K \\<br>a _{ij} ^V = w _{clip(j-i,k)} ^V \\<br>e _{ij} = \frac {(x _i W ^Q)(x _jW ^K + a _{ij} ^K) ^T}{\sqrt {d _z}}\\<br>z _i = \sum _{j=1} ^n {\alpha _{ij}(x _j W ^V + \alpha _{ij} ^v)}<br>$$</p>
<p>首先对于序列的成对关系，作者给出了限制，其认为只有距离为k的点对值得考虑，超出k则两点间影响可以固定（既clip函数裁剪下标）。从而构成了从-k到k一共2k+1个相对位置表示，将这些表示作为要学习的参数，按上式融入attention中，最终学到不同空间固定的位置表示。同时文中提到，问了减少学习参数，每个head的位置表示可以共享参数。</p>
<h5 id="Sparse-Transformers-：Generating-Long-Sequences-with-Sparse-Transformers-OpenAl2019-103"><a href="#Sparse-Transformers-：Generating-Long-Sequences-with-Sparse-Transformers-OpenAl2019-103" class="headerlink" title="Sparse Transformers ：Generating Long Sequences with Sparse Transformers[OpenAl2019, 103])"></a>Sparse Transformers ：Generating Long Sequences with Sparse Transformers<a href="https://arxiv.org/pdf/1904.10509.pdf" target="_blank" rel="noopener">[OpenAl2019, 103]</a>)</h5><p>当序列过长时，基础Transformers的attention计算将非常耗时，为了降低计算量和存储， Sparse Transformers从局部相关和全局相关两个角度来进行attention，对于局部相关，Sparse Transformers设计了一个范围k，对前后相邻k个元素进行attention。对于全局信息，Sparse Transformers则参考膨胀卷积的思路，隔l位进行抽样，因为attention迭代了很多轮，所以高层attention会逐渐融合全局信息。最终attention关注位置如下（蓝色部分）：</p>
<p><img src="/.io//Sparse_Transformers.png" alt="bubble"><br>从而极大的减少了计算时间和存储，使很长的输入序列也有很好的效果。</p>
<h5 id="Set-transformer-A-framework-for-attention-based-permutation-invariant-neural-networks-ICML2019-106"><a href="#Set-transformer-A-framework-for-attention-based-permutation-invariant-neural-networks-ICML2019-106" class="headerlink" title="Set transformer: A framework for attention-based permutation-invariant neural networks.[ICML2019, 106])"></a>Set transformer: A framework for attention-based permutation-invariant neural networks.<a href="http://proceedings.mlr.press/v97/lee19d/lee19d.pdf" target="_blank" rel="noopener">[ICML2019, 106]</a>)</h5><p>针对输入不是序列而是set（顺序无关），作者提出了一种另一种减少self-attention计算复杂度的思路。其中，有趣的点是设计了Inducing point。传统Attention输入一般为Q、K、V三部分，而这三部分往往来自于真实数据或特征，本文设计了Inducing point，即将query向量变成参数，从而学出要查询的特征。通俗来讲，例如：想利用attention从众多图片中识别猫，则Inducing query可能就会学出猫的特征。这一思路扩展了attention应用的方式，很有借鉴意义。</p>
<h4 id="2-4-1-2-Transformer方法的应用"><a href="#2-4-1-2-Transformer方法的应用" class="headerlink" title="2.4.1.2 Transformer方法的应用"></a>2.4.1.2 Transformer方法的应用</h4><h5 id="Doubly-attentive-transformer-machine-translation-2018-107"><a href="#Doubly-attentive-transformer-machine-translation-2018-107" class="headerlink" title="Doubly attentive transformer machine translation.[2018, 107])"></a>Doubly attentive transformer machine translation.<a href="https://arxiv.org/pdf/1807.11605.pdf" target="_blank" rel="noopener">[2018, 107]</a>)</h5><p>对Transformer的简单推广，将输入扩展到图文混合，图片用CNN抽取特征，文本仍用Transformer encoder 对与decoder attention部分使用文本和图片特征一起做attention。</p>
<h5 id="Input-Combination-Strategies-for-Multi-Source-Transformer-Decoder-2018-106"><a href="#Input-Combination-Strategies-for-Multi-Source-Transformer-Decoder-2018-106" class="headerlink" title="Input Combination Strategies for Multi-Source Transformer Decoder[2018, 106])"></a>Input Combination Strategies for Multi-Source Transformer Decoder<a href="https://www.aclweb.org/anthology/W18-6326.pdf" target="_blank" rel="noopener">[2018, 106]</a>)</h5><p>总结了Transformer融合多模型特征的方法，主要分成四种：serial、parallel、flat、hierarchical。serial为顺序融合，即依次对不同特征进行attention，结果作为下一个Attention query。parallel是各自做attention然后叠加到一起。flat使用concat叠加特征KV向量，然后一起做attention。Hierarchical则个字attention，然后再用attention对每个模型输出汇总。以下为不同方法试验结果。</p>
<p><img src="/.io//Input-Combination-Strategies-for-Multi-Source-Transformer-Decoder.png" alt="bubble">  </p>
<h5 id="Input-Combination-Strategies-for-Multi-Source-Transformer-Decoder-2019-106"><a href="#Input-Combination-Strategies-for-Multi-Source-Transformer-Decoder-2019-106" class="headerlink" title="Input Combination Strategies for Multi-Source Transformer Decoder[2019, 106])"></a>Input Combination Strategies for Multi-Source Transformer Decoder<a href="https://www.aclweb.org/anthology/P19-1601.pdf" target="_blank" rel="noopener">[2019, 106]</a>)</h5><p>充分利用Transformer的encoder和decoder结构，来解决文本风格切换的问题。对于输入假设有k种风格，则对于每个风格有各自的语料库，由于不同风格之间文本缺少对应关系，因此作者对Transformer进行改进设计了一种类似GAN的结构。作者将风格迁移这个任务进行分解，其实他有三个子任务，一是语言本身，即语言流利； 二是语义的保留；三是风格的迁移。      </p>
<ol>
<li>对于语言本身的流利，采用了原文句子输入到输入的auto encoder-decoder的方式，即对输入编码，再解码，loss 则是生成句子和原来句子的差别的交叉熵。区别于其他的工作，这里采用了transformer, 其本身就是堆叠的自编码器和解码器。  </li>
<li>语义的保留，作者采用的方式是，对于 一个句子x，加上风格s’ , 编码后的结果fe( x , s’)作为输入，基于原风格s，进行重建，生成x。    </li>
<li>对于子任务3，则是借助一个判别网络 , 它可以判断输入fe( x , s’) 的风格 s。 对于这个判别网络的训练，作者采用了两个loss， 一个是条件判别的loss，这个类似于GAN ，其实就是对于原始输入x ，和基于原风格生成的fe( x , s) 输出正标签，而对于其他风格的生成fe( x , s’)输出负。 另一种多标签判别的训练，则是输出语句的风格标签，共K+1个（伪生成语句的标签是0）。</li>
</ol>
<p>#####Hierarchical Transformer: Hierarchical transformers for multi-document summarization<a href="https://www.aclweb.org/anthology/P19-1601.pdf" target="_blank" rel="noopener">[ACL2019, 110]</a>)<br>针对多文本摘要问题，有由于输入是多个文档，因此对于端到端模型而言，输入太过庞大。本文通过先用无监督评估标题和文本段落匹配度评分（LSTM编码标题和段落并各自pooling压缩后计算匹配得分）选出top K的段落。然后输入改进的Transformer模型，对于Transformer模型作者Encoding加入了层次Encoding方法，即现对段落内部进行MH-Attention得到表示，再使用Multi-head Pooling（类似muti-head attention，投影到不同空间后计算attention，将一整段词汇融合成多头段落表示）得到段落多头表示，再利用attention计算融合多个段落，得到新的段落表达，并将段落表达融合进段落的每一个词中（同一段内融合的段落表达是相同的），其余部分和普通attention一样。作者利用这种分层的attention，即解决了多个文档输入过长的问题，同时也是每一个词都融合了各自段落或各自文档的信息。</p>
<h5 id="HighWay-Recurrent-Transformer：Learning-multi-level-information-for-dialogue-response-selection-by-highway-recurrent-transformer-2019-111"><a href="#HighWay-Recurrent-Transformer：Learning-multi-level-information-for-dialogue-response-selection-by-highway-recurrent-transformer-2019-111" class="headerlink" title="HighWay Recurrent Transformer：Learning multi-level information for dialogue response selection by highway recurrent transformer[2019, 111])"></a>HighWay Recurrent Transformer：Learning multi-level information for dialogue response selection by highway recurrent transformer<a href="https://arxiv.org/pdf/1903.08953.pdf" target="_blank" rel="noopener">[2019, 111]</a>)</h5><p>面向对话系统场景，对于对话系统而言，一个问题的答案和前文应答与前问问题可能都有关联，因此输入是多个句子序列，由于是句子序列，一方面使用传统Transformer输入可能会非常长，另一方面，完整的意思表达往往是以句子为一个粒度，因此仅使用之前的位置编码可能很难完全编码这种位置和句子粒度的空间信息，因此作者设计了 HighWay Recurrent Transformer网络，来优化这一问题。其具体框架如下：<br><img src="/.io//HighWay_Recurrent_Transformer.png" alt="bubble">  </p>
<p>首先从整体来讲，作者对Transformer Encoder进行了改进，对每一个问答段落，作者使用了一个Transformer Encoder Blocker进行句内编码，然后将encoder输出送入Highway Attention。Highway Attention类似于RNN的一个cell，由于结构类似RNN所以RNN具有的问题（如梯度爆炸，长期记忆消失等问题）Highway Attention可能也具有，因此模仿LSTM这里也加入了门的概念（参考了Highway Network，对输入和上一步状态加权），这里Highway Attention由两部分attention构成，一个是以上一轮输出作为KV，本轮输入作为Q计算状态的co-attention，另一部分是本轮输入的self-Attention，这里作者将权重计算融入了co&amp;self-attention权重计算中。具体公式可以参考链接中的原文。</p>
<h5 id="Lattice-Based-Transformer-：Lattice-based-transformer-encoder-for-neural-machine-translation-2019-111"><a href="#Lattice-Based-Transformer-：Lattice-based-transformer-encoder-for-neural-machine-translation-2019-111" class="headerlink" title="Lattice-Based Transformer ：Lattice-based transformer encoder for neural machine translation [2019, 111])"></a>Lattice-Based Transformer ：Lattice-based transformer encoder for neural machine translation <a href="https://www.aclweb.org/anthology/P19-1298.pdf" target="_blank" rel="noopener">[2019, 111]</a>)</h5><p>针对机器翻译，重构了输入与attention部分attention，将同一句不同分词利用Lattices，构成词的网络结构，并引入Lattice Positional Encoding编码位置，同时将所有可能的序列输入Transformer，进行attention 时同时考虑词间边的关系。</p>
<h5 id="Transformer-TTS-Network-：Neural-speech-synthesis-with-transformer-network-AAAI2019-113"><a href="#Transformer-TTS-Network-：Neural-speech-synthesis-with-transformer-network-AAAI2019-113" class="headerlink" title="Transformer TTS Network ：Neural speech synthesis with transformer network[AAAI2019, 113])"></a>Transformer TTS Network ：Neural speech synthesis with transformer network<a href="https://arxiv.org/pdf/1809.08895v3.pdf" target="_blank" rel="noopener">[AAAI2019, 113]</a>)</h5><p>Transformer 在Text to speech上的应用，借鉴了Tacotron2 ，decoder输入为音素。其框架如下，声音部分使用cnn进行编码。<br><img src="/.io//TTS_Transformer.png" alt="bubble">  </p>
<h5 id="Phrase-Based-Attention-ICLR2019-114"><a href="#Phrase-Based-Attention-ICLR2019-114" class="headerlink" title="Phrase-Based Attention[ICLR2019, 114])"></a>Phrase-Based Attention<a href="https://arxiv.org/pdf/1810.03444.pdf" target="_blank" rel="noopener">[ICLR2019, 114]</a>)</h5><p>之前的模型只从单个词角度进行学习，本文引入了通过Ngram引入词组，不同的n构成一系列词组向量然后使用MutiHead attention融合所有n的信息。其中对于Key的表示，作者没有直接使用投影矩阵相乘，而是使用卷积的，是query查询卷积核学出的特征，对于不同的n，作者分布使用各自的卷积核，然后得到的key拼接到一起（使用卷积时因为要设置窗口，所以可以更关注相邻的特征）。</p>
<h5 id="BERT-：Bert-Pre-training-of-deep-bidirectional-transformers-for-language-understanding-2019-115"><a href="#BERT-：Bert-Pre-training-of-deep-bidirectional-transformers-for-language-understanding-2019-115" class="headerlink" title="BERT ：Bert: Pre-training of deep bidirectional transformers for language understanding[2019, 115])"></a>BERT ：Bert: Pre-training of deep bidirectional transformers for language understanding<a href="https://www.aclweb.org/anthology/P19-1298.pdf" target="_blank" rel="noopener">[2019, 115]</a>)</h5><p>BERT在Transformer基础上，使用迁移学习方法提升效果，与其说是改进不如说是对Transformer encoder在不同场景的应用。其框架如下图，其中Trm是Transformer Blocker，可以看到BERT和GPT与ELMo对比。ELMo使用了双向LSTM，GPT则使用单向+Transformer做cell，这两种方法都暗含了状态转移与访问顺序，而BERT将Transformer扩展到了双向。<br><img src="/.io//BERT1.png" alt="bubble"><br>对于单纯将扩展为双向Transformer的架构（就如下图BERT描述的框架）实际上就是将Transformer全链接，然而这就产生了问题，在预训练时，无法正常预测下一个位置的值，因为是全联接，所以模型会看到要预测的值。因此作者引入了Mask Language Model(MLM)和Next sentence order(NSP)两种方法，具体做法是对词序列，可以给其中词汇随机替换成mask（类似完形填空），对于句子预测两个句子是不是下一句的关系，从文档生成连续句和非连续句。<br>在Fine-Tuning，可以根据不同任务设计输入序列，以及输出的应用。</p>
<h5 id="GPT-：Improving-language-understanding-by-generative-pre-training-2019-120"><a href="#GPT-：Improving-language-understanding-by-generative-pre-training-2019-120" class="headerlink" title="GPT ：Improving language understanding by generative pre-training [2019, 120])"></a>GPT ：Improving language understanding by generative pre-training <a href="https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf" target="_blank" rel="noopener">[2019, 120]</a>)</h5><p>GPT模型在BERT框架中有提及，其也是对transformers的改进与应用，和bert一样只是用了encoder部分做网络cell，但因为整体模型仍采用自回归模式，即新位置的学习只使用当前位置之前的输入，所以encoder部分要加入mask（又或者说，使用了去掉encoder attention sublayer的decoder）</p>
<h5 id="GPT-2-：-Language-models-are-unsupervised-multitask-learners-OpenAI2019-116"><a href="#GPT-2-：-Language-models-are-unsupervised-multitask-learners-OpenAI2019-116" class="headerlink" title="GPT-2 ： Language models are unsupervised multitask learners[OpenAI2019, 116])"></a>GPT-2 ： Language models are unsupervised multitask learners<a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" target="_blank" rel="noopener">[OpenAI2019, 116]</a>)</h5><p>GPT-2依然沿用GPT单向transformer的模式，只不过做了一些改进与改变。首先 GPT-2去掉了fine-tuning层，不再针对不同任务分别进行微调建模。而因为去掉了fine-tuning层，为了能识别特定问题的目的，其做了如下改动：</p>
<ol>
<li>首先需要增加数据集，GPT-2收集了更加广泛、数量更多的语料组成高质量文本数据集，该数据集包含800万个网页，大小为40G。</li>
<li>增加网络参数，GPT-2将Transformer堆叠的层数增加到48层，隐层的维度为1600，参数量更是达到了15亿。</li>
<li>调整LN：将layer normalization放到每个sub-block之前，并在最后一个Self-attention后再增加一个layer normalization。</li>
<li>加入任务引导提示词，如：“TL;DR:”，GPT-2模型就会知道是做摘要工作了，输入的格式就是文本+提示词，从而能自动学出目标。</li>
</ol>
<h5 id="GPT-3-：Language-models-are-few-shot-learners-OpenAI2020-117"><a href="#GPT-3-：Language-models-are-few-shot-learners-OpenAI2020-117" class="headerlink" title="GPT-3 ：Language models are few-shot learners[OpenAI2020, 117])"></a>GPT-3 ：Language models are few-shot learners<a href="https://www.aclweb.org/anthology/P19-1298.pdf" target="_blank" rel="noopener">[OpenAI2020, 117]</a>)</h5><p>GPT-3在GPT2的基础上再次扩张数据集（45TB）与参数（1750亿），同时改进了attention，当输入过多时，加入alternating dense和locally banded sparse attention，增强局部视野。</p>
<h5 id="Image-Transformer-ICML2018-118"><a href="#Image-Transformer-ICML2018-118" class="headerlink" title="Image Transformer[ICML2018, 118])"></a>Image Transformer<a href="https://arxiv.org/pdf/1802.05751.pdf" target="_blank" rel="noopener">[ICML2018, 118]</a>)</h5><p>将语言模型迁移到了图像补全的问题，其场景非常相似，文章中图像补全问题每个像素是根据之前其他像素和状态生成的，这和语言模型生成新词场景类似，因此作者将Transformer推广到了该场景。对于图像而言，输入像素很多，因此作者使用局部自注意力，限制了attention关注范围。</p>
<h3 id="2-4-2-Graph-Attention-Networks"><a href="#2-4-2-Graph-Attention-Networks" class="headerlink" title="2.4.2 Graph Attention Networks"></a>2.4.2 Graph Attention Networks</h3><h2 id="2-4-Attention-today"><a href="#2-4-Attention-today" class="headerlink" title="2.4 Attention today"></a>2.4 Attention today</h2><p>现如今使用混合模型逐渐成为深度学习领域使用Attention的主要的发展方向。基于Transformer，GATs，MemNet的工作被不断翻新并使用在各个应用中。<br>双曲空间：Hyperbolic Attention Networks (HAN) ，Hyperbolic Graph Attention Networks (GHN) 通过将研究空间从欧式空间转换到双曲空间，从而解决了数据与embedding空间指数扩张的问题。同时双去空间用于深度学习的研究从2019年开始也逐渐成为一个新的热点领域。<br>Graph Attention：从2019年，GATs在图上应用attention受到了广泛的关注，应用attention，模型可以学习到更加复杂的关系。同时其他的研究如MGNs 和 TGNs 使用了memory模块做结合也有不错的效果。</p>
<p>对RNN结构的探索：到2020年底，仍有两个工作在关注RNN结构的探索，其中一个是通过计算得到RNN的cell长度，即ACT（adaptive computation time ），ACT最开始出现于2016年，由DeepMind提出，其将RNN一个时刻的单个Cell该进成同时刻多cell迭代\(h _t ^l =Cell(h _t ^{l-1}, x _t ^l ) \)，和多层RNN不同，每层输入都为x，为了加入迭代轮数信息，没多增加一次迭代，输入x会叠加一个delta值（这里使用10标记位），再通过每轮输出+1层网络计算权重，当大于某个权重时，停止计算。而最新的DACT在ACT基础上做了改进，提出了全程可微分的E2E模型。另一个则为关注top-down 和 bottom-up在RNN传递中的作用其文章如下：</p>
<h5 id="Learning-to-combine-top-down-and-bottom-up-signals-in-recurrent-neural-networks-with-attention-over-modules-ICML2020-125"><a href="#Learning-to-combine-top-down-and-bottom-up-signals-in-recurrent-neural-networks-with-attention-over-modules-ICML2020-125" class="headerlink" title="Learning to combine top-down and bottom-up signals in recurrent neural networks with attention over modules[ICML2020, 125])"></a>Learning to combine top-down and bottom-up signals in recurrent neural networks with attention over modules<a href="https://arxiv.org/pdf/2006.16981.pdf" target="_blank" rel="noopener">[ICML2020, 125]</a>)</h5><p>Bottom-up指直接从句子中观测到的信息，top-down则指基于过往经验和short-term memory得到的预测。针对这两种情况作者设计了如下框架：<br><img src="/.io//top-down_bottom-up.png" alt="bubble"><br>从整体来看，其框架是对RNN进行了改进，特征除了从底层传递到上层，从t-1时刻传递到t时刻外，还加入了从t-1时刻到上层传递到t时刻下层的边（即引入了作者所谓的TopDown的考量）。<br>对于一层而言，作者使用了Recurrent Independent Mechanisms，和传统rnn不同，一个rnn层是多个独立的cell共同组成。<br>对于这些cell，一方面做着设计了Selective Activation，将t-1的多个cell输出做query，零向量和cell的输入[Zero,xt] 做KV，进行attention，这样可以得到输入和零向量的attention评分，从而得到该层与输入的关系，利用评分选出m个关系最大的cell，参与状态更新，其余的保持t-1状态。<br>另一方面，作者还设计了cell间的Communication，这主要用当前时刻cell的输出做self-attention，融合彼此间特征，然后将融合后的结果叠加到当前输出（只有Selective Activation激活的cell需要更新）。<br>对于层间传递而言，和LSTM这一类不同，作者没有仅将上一层做输入，而是使用了当前层t-1时刻状态 \(h _{t-1} ^l\) 做Query，\([ \emptyset, h _t ^{l-1}, h _{t-1} ^{l+1}]\)做KV，从而融合上一层与下一层的状态，这里同样引入了0向量，可以很好的减轻当 \( h _t ^{l-1}\)  与\( h _{t-1} ^{l+1}\) 和当前层上一时刻状态无关时，多余的特征引入。</p>
<h1 id="3-Attention-Mechanisms"><a href="#3-Attention-Mechanisms" class="headerlink" title="3 Attention Mechanisms"></a>3 Attention Mechanisms</h1><p>总体来说，Attention可以分为3种，即： soft attention (global attention), hard attention (local attention), 和 self-attention (intra-attention)。   </p>
<p><strong>Soft Attention</strong>： 是在每一个输入上做加权求和，其权重为0到1之间，通过权重控制在每一个input上给予多少关注，权重通常来自于深度学习中输入与目标的相关性。一般使用softmax作为Attention层权重计算函数，从而使模型更加确定并且可微分。soft attention可以于作空域和频域，对于空域，其主要用于提取特征或为最相关的特征施加更大权重。对于空域，用来对每一个时间窗口内特征进行加权，计算不同窗口内特征贡献度。虽然softmax是确定可微的，但当输入很大时，softmax计算量也很高。</p>
<p><strong>Hard Attention</strong> ：Hard Attention决定了模型中的一部分输入是非要被考虑到，对于权重的分配为1/0二值，其过程是不可微分的。其过程主要是对一系列输入决定那一部分是需要被考虑的，对于频域，举例而言，模型需要决定下一步是否要考虑当前输入信息的某一部分。显然对于这种区域选择，是没有一个正确与否的确定性评判的，因此，Hard Attention往往是基于统计过程。也正是因为模型是不可微的，因此强化学习技术通常会需要使用Hard Attention来寻李娜模型。对于算法时间复杂度而言，Hard Attention要低于Soft Attention。</p>
<p><strong>Self Attention</strong> ：Self-Attention严格来说并不是一种新的分类，只是对Attention而言QKV这这里是相同的，也正因为此，Self-Attention一般可以用矩阵方法高度并行。</p>
<h1 id="4-Attention-based-Classic-Deep-Learning-Architectures"><a href="#4-Attention-based-Classic-Deep-Learning-Architectures" class="headerlink" title="4 Attention-based Classic Deep Learning Architectures"></a>4 Attention-based Classic Deep Learning Architectures</h1><h2 id="4-1-Attention-based-Convolutional-Neural-Networks-CNNs"><a href="#4-1-Attention-based-Convolutional-Neural-Networks-CNNs" class="headerlink" title="4.1 Attention-based Convolutional Neural Networks (CNNs)"></a>4.1 Attention-based Convolutional Neural Networks (CNNs)</h2><h2 id="4-2-Attention-based-Recurrent-Neural-Networks-RNNs"><a href="#4-2-Attention-based-Recurrent-Neural-Networks-RNNs" class="headerlink" title="4.2 Attention-based Recurrent Neural Networks (RNNs)"></a>4.2 Attention-based Recurrent Neural Networks (RNNs)</h2><h2 id="4-3-Attention-based-Generative-Models"><a href="#4-3-Attention-based-Generative-Models" class="headerlink" title="4.3 Attention-based Generative Models"></a>4.3 Attention-based Generative Models</h2><h1 id="5-Applications"><a href="#5-Applications" class="headerlink" title="5 Applications"></a>5 Applications</h1><h2 id="5-1-Natural-Language-Processing-NLP"><a href="#5-1-Natural-Language-Processing-NLP" class="headerlink" title="5.1 Natural Language Processing (NLP)"></a>5.1 Natural Language Processing (NLP)</h2><h2 id="5-2-Computer-Vision-CV"><a href="#5-2-Computer-Vision-CV" class="headerlink" title="5.2 Computer Vision (CV)"></a>5.2 Computer Vision (CV)</h2><h2 id="5-3-Multimodal-Tasks-CV-NLP"><a href="#5-3-Multimodal-Tasks-CV-NLP" class="headerlink" title="5.3 Multimodal Tasks (CV/NLP)"></a>5.3 Multimodal Tasks (CV/NLP)</h2><h2 id="5-4-Recommender-Systems-RS"><a href="#5-4-Recommender-Systems-RS" class="headerlink" title="5.4 Recommender Systems (RS)"></a>5.4 Recommender Systems (RS)</h2><h2 id="5-5-Reinforcement-Learning-RL"><a href="#5-5-Reinforcement-Learning-RL" class="headerlink" title="5.5 Reinforcement Learning (RL)"></a>5.5 Reinforcement Learning (RL)</h2><h2 id="5-6-Robotics"><a href="#5-6-Robotics" class="headerlink" title="5.6 Robotics"></a>5.6 Robotics</h2><h2 id="5-7-Interpretability"><a href="#5-7-Interpretability" class="headerlink" title="5.7 Interpretability"></a>5.7 Interpretability</h2><h1 id="6-Trends-and-Opportunities"><a href="#6-Trends-and-Opportunities" class="headerlink" title="6 Trends and Opportunities"></a>6 Trends and Opportunities</h1><h2 id="6-1-End-To-End-Attention-models"><a href="#6-1-End-To-End-Attention-models" class="headerlink" title="6.1 End-To-End Attention models"></a>6.1 End-To-End Attention models</h2><h2 id="6-2-Learning-Multimodality"><a href="#6-2-Learning-Multimodality" class="headerlink" title="6.2 Learning Multimodality"></a>6.2 Learning Multimodality</h2><h2 id="6-3-Cognitive-Elements"><a href="#6-3-Cognitive-Elements" class="headerlink" title="6.3 Cognitive Elements"></a>6.3 Cognitive Elements</h2><h2 id="6-4-Computer-Vision"><a href="#6-4-Computer-Vision" class="headerlink" title="6.4 Computer Vision"></a>6.4 Computer Vision</h2><h2 id="6-5-Capsule-Neural-Network"><a href="#6-5-Capsule-Neural-Network" class="headerlink" title="6.5 Capsule Neural Network"></a>6.5 Capsule Neural Network</h2><h2 id="6-6-Neural-Symbolic-Learning-and-Reasoning"><a href="#6-6-Neural-Symbolic-Learning-and-Reasoning" class="headerlink" title="6.6 Neural-Symbolic Learning and Reasoning"></a>6.6 Neural-Symbolic Learning and Reasoning</h2><h2 id="6-7-Incremental-Learning"><a href="#6-7-Incremental-Learning" class="headerlink" title="6.7 Incremental Learning"></a>6.7 Incremental Learning</h2><h2 id="6-8-Credit-Assignment-Problem-CAP"><a href="#6-8-Credit-Assignment-Problem-CAP" class="headerlink" title="6.8 Credit Assignment Problem (CAP)"></a>6.8 Credit Assignment Problem (CAP)</h2><h2 id="6-9-Attention-and-Interpretability"><a href="#6-9-Attention-and-Interpretability" class="headerlink" title="6.9 Attention and Interpretability"></a>6.9 Attention and Interpretability</h2><h2 id="6-10-Unsupervised-Learning"><a href="#6-10-Unsupervised-Learning" class="headerlink" title="6.10 Unsupervised Learning"></a>6.10 Unsupervised Learning</h2><h2 id="6-11-New-Tasks-and-Robotics"><a href="#6-11-New-Tasks-and-Robotics" class="headerlink" title="6.11 New Tasks and Robotics"></a>6.11 New Tasks and Robotics</h2><h2 id="7-Conclusions"><a href="#7-Conclusions" class="headerlink" title="7 Conclusions"></a>7 Conclusions</h2><hr>
<h2 id="Mentioned-But-Not-Discribe"><a href="#Mentioned-But-Not-Discribe" class="headerlink" title="Mentioned But Not Discribe"></a>Mentioned But Not Discribe</h2><p><a name="mattnet"> </a><br>**MAttNet: Modular Attention Network for Referring Expression Comprehension<a href="https://https//arxiv.org/pdf/1801.08186.pdf" target="_blank" rel="noopener">[CVPR2018, 462]</a>**：<br>Expression Comprehension 问题，作者使用Faster RCNN做目标检测，对检测出的目标别从Subject、location、relationship三方面进行评分，最终汇总。其具体过程如下：<br><img src="/.io//mattnet.jpeg" alt="bubble"><br>作者使用LSTM对文本进行embedding，引入attention机制，学出Subject、location、relationship三种不同的embedding向量。得到向量后，分别利用不同方式与图片object进行match得到分数，最终加权求和。</p>
<hr>
<h2 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h2><p><a name="BLEU"> </a></p>
<h3 id="BLEU（bilingual-evaluation-understudy）："><a href="#BLEU（bilingual-evaluation-understudy）：" class="headerlink" title="BLEU（bilingual evaluation understudy）："></a>BLEU（bilingual evaluation understudy）：</h3><p>对翻译句和参考句作N-gram切分，计算不同N下每一个N-gram的匹配度。<br>$$<br>BP = 1\ if\ lc &gt;ls\ else\ e ^{1-ls/lc}\\<br>BLEU = BP * exp(\sum _n W _n P _n)<br>$$<br>其中BP为长度惩罚因子，译文长度小于参考长度时叠加计算惩罚因子，防止只翻译出了部分句子且翻译的比较准确时，匹配度依然会很高的问题。P为N-gram的匹配度，分母为参考答案所有N-gram词汇的出现频率和，分子为翻译的N-gram可能词汇的出现频率和（翻译相同词汇频率&gt;答案相同词汇，取答案的词汇频率，以保证p&lt;=1），W为当前N下权重，一般为1/N。BLEU一般取N&lt;=4.</p>
<h3 id="ConvLSTM"><a href="#ConvLSTM" class="headerlink" title="ConvLSTM"></a>ConvLSTM</h3><p>RNN输入变为二维矩阵，参数仍为2维矩阵（全链接变卷积）</p>
<h3 id="dilated-convolution"><a href="#dilated-convolution" class="headerlink" title="dilated convolution"></a>dilated convolution</h3><p>空洞卷积，例：空洞率为2，3*3卷积核，对7*7输入进行空洞卷积，输出为3*3。进行卷积时不对相邻元素进行卷积而是跳过空洞率-1位取元素。好处是不做pooling，仍能扩大感受视野，输出不会因为pooling为缩小，适合稠密预测</p>
<h3 id="dilated-convolution-1"><a href="#dilated-convolution-1" class="headerlink" title="dilated convolution"></a>dilated convolution</h3><h3 id="SE-Block-（Sequeze-and-Excitation）"><a href="#SE-Block-（Sequeze-and-Excitation）" class="headerlink" title="SE Block （Sequeze and Excitation）"></a>SE Block （Sequeze and Excitation）</h3><p>SE的出现是为了解决在卷积池化过程中feature map的不同通道所占的重要性不同带来的损失问题。在传统的卷积池化过程中，默认feature map的每个通道是同等重要的，而在实际的问题中，不同通道的重要性是有差异的，具体问题具体看待。具体弥补方式是为每个通道增加权重，首先对每个通道进行全局pooling(可以算AVG或Max等，可以根据不同pooling策略生成多个特征)生成每个通道的pooling特征，然后用双层MLP进行通道权重学习（第一层进行进行降维+ReLU，第二次还原维度+Sigmoid，增加泛化，学习更多非线性复杂关系），学好的权重叠加到原始输入中。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Attention/" rel="tag"># Attention</a>
          
            <a href="/tags/Survey/" rel="tag"># Survey</a>
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
            <a href="/tags/Neural-Networks/" rel="tag"># Neural Networks</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/blog/2020/11/12/【Paper-Reading】A-Survey-on-Network-Embedding/" rel="next" title="【Paper Reading】A Survey on Network Embedding">
                <i class="fa fa-chevron-left"></i> 【Paper Reading】A Survey on Network Embedding
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpeg" alt="Zhiqi Liu">
            
              <p class="site-author-name" itemprop="name">Zhiqi Liu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/liuzhiqi" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:yourname@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Introduction"><span class="nav-number">1.</span> <span class="nav-text">1 Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-Overview"><span class="nav-number">2.</span> <span class="nav-text">2 Overview</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#RNNSearch：-Neural-machine-translation-by-jointly-learning-to-align-and-translate-2019-115"><span class="nav-number">2.0.0.0.1.</span> <span class="nav-text">RNNSearch： Neural machine translation by jointly learning to align and translate [2019, 115]</span></a></li></ol></li></ol><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-Attentional-interfaces"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 Attentional interfaces</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1-Attentional-interfaces-in-NLP"><span class="nav-number">2.1.1.</span> <span class="nav-text">2.1.1 Attentional interfaces in NLP</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Listen-Attend-and-Spell-2015-48"><span class="nav-number">2.1.1.0.1.</span> <span class="nav-text">Listen, Attend and Spell [2015, 48]:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Grammar-as-a-Foreign-Language-2015-49"><span class="nav-number">2.1.1.0.2.</span> <span class="nav-text">Grammar as a Foreign Language [2015, 49]:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#BiDAF：Bidirectional-attention-flow-for-machine-comprehension-2016-51"><span class="nav-number">2.1.1.0.3.</span> <span class="nav-text">BiDAF：Bidirectional attention flow for machine comprehension[2016, 51]:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#BiDAF：Bidirectional-attention-flow-for-machine-comprehension-2016-52"><span class="nav-number">2.1.1.0.4.</span> <span class="nav-text">BiDAF：Bidirectional attention flow for machine comprehension[2016, 52]:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Dynamic-coattention-networks-for-question-answering-2016-53"><span class="nav-number">2.1.1.0.5.</span> <span class="nav-text">Dynamic coattention networks for question answering[2016, 53]:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Ptr-Net：Pointer-Networks-NIPS2015-54"><span class="nav-number">2.1.1.0.6.</span> <span class="nav-text">Ptr-Net：Pointer Networks[NIPS2015, 54]:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Ptr-Net：Pointer-Networks-ACL2017-55"><span class="nav-number">2.1.1.0.7.</span> <span class="nav-text">Ptr-Net：Pointer Networks[ACL2017, 55]:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Fusionnet-Fusing-via-fully-aware-attention-with-application-to-machine-comprehension-2017-56"><span class="nav-number">2.1.1.0.8.</span> <span class="nav-text">Fusionnet: Fusing via fully-aware attention with application to machine comprehension[2017, 56]:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Effective-Approaches-to-Attention-based-Neural-Machine-Translation-2015-57"><span class="nav-number">2.1.1.0.9.</span> <span class="nav-text">Effective Approaches to Attention-based Neural Machine Translation[2015, 57]:</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-2-Attentional-interfaces-in-Computer-Vision"><span class="nav-number">2.1.2.</span> <span class="nav-text">2.1.2 Attentional interfaces in Computer Vision</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#RAM：Recurrent-models-of-visual-attention-2014-46"><span class="nav-number">2.1.2.0.1.</span> <span class="nav-text">RAM：Recurrent models of visual attention[2014, 46]:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#STN：Spatial-Transformer-Networks-2015-59"><span class="nav-number">2.1.2.0.2.</span> <span class="nav-text">STN：Spatial Transformer Networks[2015, 59]:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Draw-A-recurrent-neural-network-for-image-generation-2015-36"><span class="nav-number">2.1.2.0.3.</span> <span class="nav-text">Draw: A recurrent neural network for image generation[2015, 36]:</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-Multimodality"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 Multimodality</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-图像到文本"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.1 图像到文本</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Show-attend-and-tell-Neural-image-caption-generation-with-visual-attention-ICML2015-45"><span class="nav-number">2.2.1.0.1.</span> <span class="nav-text">Show, attend and tell: Neural image caption generation with visual attention[ICML2015, 45]:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#monocular-RGB-D-images-Global-context-aware-attention-lstm-networks-for-3d-action-recognition-CVPR2017-67"><span class="nav-number">2.2.1.0.2.</span> <span class="nav-text">monocular/RGB-D images Global context-aware attention lstm networks for 3d action recognition[CVPR2017, 67]:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Attention-in-convolutional-lstm-for-gesture-recognition-NIPS2018-68"><span class="nav-number">2.2.1.0.3.</span> <span class="nav-text">Attention in convolutional lstm for gesture recognition[NIPS2018, 68]:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Attention-in-convolutional-lstm-for-gesture-recognition-ECCV2018-69"><span class="nav-number">2.2.1.0.4.</span> <span class="nav-text">Attention in convolutional lstm for gesture recognition[ECCV2018, 69]:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Hyperspectral-images-classification-based-on-dense-convolutional-networks-with-spectral-wise-attention-mechanism-2019-71"><span class="nav-number">2.2.1.0.5.</span> <span class="nav-text">Hyperspectral images classification based on dense convolutional networks with spectral-wise attention mechanism[2019, 71]:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Scene-classification-with-recurrent-attention-of-vhr-remote-sensing-images-2018-72"><span class="nav-number">2.2.1.0.6.</span> <span class="nav-text">Scene classification with recurrent attention of vhr remote sensing images[2018, 72]:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#audio-video-Attention-based-multimodal-fusion-for-video-description-ICCV2017-74"><span class="nav-number">2.2.1.0.7.</span> <span class="nav-text">audio-video Attention-based multimodal fusion for video description[ICCV2017, 74]:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Multiattention-recurrent-network-for-human-communication-comprehension-AAAI2018-77"><span class="nav-number">2.2.1.0.8.</span> <span class="nav-text">Multiattention recurrent network for human communication comprehension[AAAI2018, 77]:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Relational-recurrent-neural-networks-NIPS2018-78-："><span class="nav-number">2.2.1.0.9.</span> <span class="nav-text">Relational recurrent neural networks [NIPS2018, 78]：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Neural-Multimodal-Belief-Tracker-with-Adaptive-Attention-for-Dialogue-Systems-WWW2019-79-："><span class="nav-number">2.2.1.0.10.</span> <span class="nav-text">Neural Multimodal Belief Tracker with Adaptive Attention for Dialogue Systems [WWW2019, 79]：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Dual-attention-networks-for-multimodal-reasoning-and-matching-CVPR2017-80-："><span class="nav-number">2.2.1.0.11.</span> <span class="nav-text">Dual attention networks for multimodal reasoning and matching [CVPR2017, 80]：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Bi-directional-Spatial-Semantic-Attention-Networks-for-Image-Text-Matching-TIP2018-81-："><span class="nav-number">2.2.1.0.12.</span> <span class="nav-text">Bi-directional Spatial-Semantic Attention Networks for Image-Text Matching[TIP2018, 81]：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Pointing-novel-objects-in-image-captioning-CVPR2019-82-："><span class="nav-number">2.2.1.0.13.</span> <span class="nav-text">Pointing novel objects in image captioning[CVPR2019, 82]：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Improving-referring-expression-grounding-with-cross-modal-attention-guided-erasing-CVPR2019-84-："><span class="nav-number">2.2.1.0.14.</span> <span class="nav-text">Improving referring expression grounding with cross-modal attention-guided erasing[CVPR2019, 84]：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-Attention-augmented-memory"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 Attention-augmented memory</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Neural-Turing-Machine-NTM-GoogleDeepMind2014-86-："><span class="nav-number">2.3.0.0.1.</span> <span class="nav-text">Neural Turing Machine (NTM)[GoogleDeepMind2014, 86]：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Memory-Networks-Facebook2014-38-："><span class="nav-number">2.3.0.0.2.</span> <span class="nav-text">Memory Networks[Facebook2014, 38]：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#End-to-end-Memory-Networks-NIPS2015-91-："><span class="nav-number">2.3.0.0.3.</span> <span class="nav-text">End-to-end Memory Networks[NIPS2015, 91]：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Video-Object-Segmentation-using-Space-Time-Memory-Networks-ICCV2019-92-："><span class="nav-number">2.3.0.0.4.</span> <span class="nav-text">Video Object Segmentation using Space-Time Memory Networks[ICCV2019, 92]：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Ask-me-anything-Dynamic-memory-networks-for-natural-language-processing（DMN）-ICML2016-93-："><span class="nav-number">2.3.0.0.5.</span> <span class="nav-text">Ask me anything: Dynamic memory networks for natural language processing（DMN）[ICML2016, 93]：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Dynamic-memory-networks-for-visual-and-textual-question-answering（DMN-）-ICML2016-88-："><span class="nav-number">2.3.0.0.6.</span> <span class="nav-text">Dynamic memory networks for visual and textual question answering（DMN+）[ICML2016, 88]：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Key-Value-Memory-Networks-for-Directly-Reading-Documents-ACL2016-95-："><span class="nav-number">2.3.0.0.7.</span> <span class="nav-text">Key-Value Memory Networks for Directly Reading Documents[ACL2016, 95]：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Memory-based-graph-networks-ICLR2020-87-："><span class="nav-number">2.3.0.0.8.</span> <span class="nav-text">Memory-based graph networks[ICLR2020, 87]：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Video-Object-Segmentation-with-Episodic-Graph-Memory-Networks-ECCV2020-89-："><span class="nav-number">2.3.0.0.9.</span> <span class="nav-text">Video Object Segmentation with Episodic Graph Memory Networks[ECCV2020, 89]：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Episodic-camn-Contextual-attention-based-memory-networks-with-iterative-feedback-for-scene-labeling-CVPR2017-90-："><span class="nav-number">2.3.0.0.10.</span> <span class="nav-text">Episodic camn: Contextual attention-based memory networks with iterative feedback for scene labeling[CVPR2017, 90]：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-End-to-end-attention-models"><span class="nav-number">2.4.</span> <span class="nav-text">2.4 End-to-end attention models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-1-Transformer-Based"><span class="nav-number">2.4.1.</span> <span class="nav-text">2.4.1 Transformer Based</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-1-1-Transformer方法的改进"><span class="nav-number">2.4.1.1.</span> <span class="nav-text">2.4.1.1 Transformer方法的改进</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Weighted-Transformer-：Weighted-transformer-network-for-machine-translation-2017-102"><span class="nav-number">2.4.1.1.1.</span> <span class="nav-text">Weighted Transformer ：Weighted transformer network for machine translation[2017, 102])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Star-transformer-NAACL2019-103"><span class="nav-number">2.4.1.1.2.</span> <span class="nav-text">Star-transformer[NAACL2019, 103])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Self-Attention-with-Relative-Position-Representations-NAACL2018-174"><span class="nav-number">2.4.1.1.3.</span> <span class="nav-text">Self-Attention with Relative Position Representations[NAACL2018, 174])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Sparse-Transformers-：Generating-Long-Sequences-with-Sparse-Transformers-OpenAl2019-103"><span class="nav-number">2.4.1.1.4.</span> <span class="nav-text">Sparse Transformers ：Generating Long Sequences with Sparse Transformers[OpenAl2019, 103])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Set-transformer-A-framework-for-attention-based-permutation-invariant-neural-networks-ICML2019-106"><span class="nav-number">2.4.1.1.5.</span> <span class="nav-text">Set transformer: A framework for attention-based permutation-invariant neural networks.[ICML2019, 106])</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-1-2-Transformer方法的应用"><span class="nav-number">2.4.1.2.</span> <span class="nav-text">2.4.1.2 Transformer方法的应用</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Doubly-attentive-transformer-machine-translation-2018-107"><span class="nav-number">2.4.1.2.1.</span> <span class="nav-text">Doubly attentive transformer machine translation.[2018, 107])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Input-Combination-Strategies-for-Multi-Source-Transformer-Decoder-2018-106"><span class="nav-number">2.4.1.2.2.</span> <span class="nav-text">Input Combination Strategies for Multi-Source Transformer Decoder[2018, 106])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Input-Combination-Strategies-for-Multi-Source-Transformer-Decoder-2019-106"><span class="nav-number">2.4.1.2.3.</span> <span class="nav-text">Input Combination Strategies for Multi-Source Transformer Decoder[2019, 106])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#HighWay-Recurrent-Transformer：Learning-multi-level-information-for-dialogue-response-selection-by-highway-recurrent-transformer-2019-111"><span class="nav-number">2.4.1.2.4.</span> <span class="nav-text">HighWay Recurrent Transformer：Learning multi-level information for dialogue response selection by highway recurrent transformer[2019, 111])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Lattice-Based-Transformer-：Lattice-based-transformer-encoder-for-neural-machine-translation-2019-111"><span class="nav-number">2.4.1.2.5.</span> <span class="nav-text">Lattice-Based Transformer ：Lattice-based transformer encoder for neural machine translation [2019, 111])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Transformer-TTS-Network-：Neural-speech-synthesis-with-transformer-network-AAAI2019-113"><span class="nav-number">2.4.1.2.6.</span> <span class="nav-text">Transformer TTS Network ：Neural speech synthesis with transformer network[AAAI2019, 113])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Phrase-Based-Attention-ICLR2019-114"><span class="nav-number">2.4.1.2.7.</span> <span class="nav-text">Phrase-Based Attention[ICLR2019, 114])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#BERT-：Bert-Pre-training-of-deep-bidirectional-transformers-for-language-understanding-2019-115"><span class="nav-number">2.4.1.2.8.</span> <span class="nav-text">BERT ：Bert: Pre-training of deep bidirectional transformers for language understanding[2019, 115])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#GPT-：Improving-language-understanding-by-generative-pre-training-2019-120"><span class="nav-number">2.4.1.2.9.</span> <span class="nav-text">GPT ：Improving language understanding by generative pre-training [2019, 120])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#GPT-2-：-Language-models-are-unsupervised-multitask-learners-OpenAI2019-116"><span class="nav-number">2.4.1.2.10.</span> <span class="nav-text">GPT-2 ： Language models are unsupervised multitask learners[OpenAI2019, 116])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#GPT-3-：Language-models-are-few-shot-learners-OpenAI2020-117"><span class="nav-number">2.4.1.2.11.</span> <span class="nav-text">GPT-3 ：Language models are few-shot learners[OpenAI2020, 117])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Image-Transformer-ICML2018-118"><span class="nav-number">2.4.1.2.12.</span> <span class="nav-text">Image Transformer[ICML2018, 118])</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-2-Graph-Attention-Networks"><span class="nav-number">2.4.2.</span> <span class="nav-text">2.4.2 Graph Attention Networks</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-Attention-today"><span class="nav-number">2.5.</span> <span class="nav-text">2.4 Attention today</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Learning-to-combine-top-down-and-bottom-up-signals-in-recurrent-neural-networks-with-attention-over-modules-ICML2020-125"><span class="nav-number">2.5.0.0.1.</span> <span class="nav-text">Learning to combine top-down and bottom-up signals in recurrent neural networks with attention over modules[ICML2020, 125])</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-Attention-Mechanisms"><span class="nav-number">3.</span> <span class="nav-text">3 Attention Mechanisms</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-Attention-based-Classic-Deep-Learning-Architectures"><span class="nav-number">4.</span> <span class="nav-text">4 Attention-based Classic Deep Learning Architectures</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-Attention-based-Convolutional-Neural-Networks-CNNs"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 Attention-based Convolutional Neural Networks (CNNs)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-Attention-based-Recurrent-Neural-Networks-RNNs"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 Attention-based Recurrent Neural Networks (RNNs)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-Attention-based-Generative-Models"><span class="nav-number">4.3.</span> <span class="nav-text">4.3 Attention-based Generative Models</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-Applications"><span class="nav-number">5.</span> <span class="nav-text">5 Applications</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-Natural-Language-Processing-NLP"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 Natural Language Processing (NLP)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-Computer-Vision-CV"><span class="nav-number">5.2.</span> <span class="nav-text">5.2 Computer Vision (CV)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-Multimodal-Tasks-CV-NLP"><span class="nav-number">5.3.</span> <span class="nav-text">5.3 Multimodal Tasks (CV/NLP)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-4-Recommender-Systems-RS"><span class="nav-number">5.4.</span> <span class="nav-text">5.4 Recommender Systems (RS)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-5-Reinforcement-Learning-RL"><span class="nav-number">5.5.</span> <span class="nav-text">5.5 Reinforcement Learning (RL)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-6-Robotics"><span class="nav-number">5.6.</span> <span class="nav-text">5.6 Robotics</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-7-Interpretability"><span class="nav-number">5.7.</span> <span class="nav-text">5.7 Interpretability</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-Trends-and-Opportunities"><span class="nav-number">6.</span> <span class="nav-text">6 Trends and Opportunities</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#6-1-End-To-End-Attention-models"><span class="nav-number">6.1.</span> <span class="nav-text">6.1 End-To-End Attention models</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-2-Learning-Multimodality"><span class="nav-number">6.2.</span> <span class="nav-text">6.2 Learning Multimodality</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-3-Cognitive-Elements"><span class="nav-number">6.3.</span> <span class="nav-text">6.3 Cognitive Elements</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-4-Computer-Vision"><span class="nav-number">6.4.</span> <span class="nav-text">6.4 Computer Vision</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-5-Capsule-Neural-Network"><span class="nav-number">6.5.</span> <span class="nav-text">6.5 Capsule Neural Network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-6-Neural-Symbolic-Learning-and-Reasoning"><span class="nav-number">6.6.</span> <span class="nav-text">6.6 Neural-Symbolic Learning and Reasoning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-7-Incremental-Learning"><span class="nav-number">6.7.</span> <span class="nav-text">6.7 Incremental Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-8-Credit-Assignment-Problem-CAP"><span class="nav-number">6.8.</span> <span class="nav-text">6.8 Credit Assignment Problem (CAP)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-9-Attention-and-Interpretability"><span class="nav-number">6.9.</span> <span class="nav-text">6.9 Attention and Interpretability</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-10-Unsupervised-Learning"><span class="nav-number">6.10.</span> <span class="nav-text">6.10 Unsupervised Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-11-New-Tasks-and-Robotics"><span class="nav-number">6.11.</span> <span class="nav-text">6.11 New Tasks and Robotics</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-Conclusions"><span class="nav-number">6.12.</span> <span class="nav-text">7 Conclusions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Mentioned-But-Not-Discribe"><span class="nav-number">6.13.</span> <span class="nav-text">Mentioned But Not Discribe</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Appendix"><span class="nav-number">6.14.</span> <span class="nav-text">Appendix</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#BLEU（bilingual-evaluation-understudy）："><span class="nav-number">6.14.1.</span> <span class="nav-text">BLEU（bilingual evaluation understudy）：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ConvLSTM"><span class="nav-number">6.14.2.</span> <span class="nav-text">ConvLSTM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dilated-convolution"><span class="nav-number">6.14.3.</span> <span class="nav-text">dilated convolution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dilated-convolution-1"><span class="nav-number">6.14.4.</span> <span class="nav-text">dilated convolution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SE-Block-（Sequeze-and-Excitation）"><span class="nav-number">6.14.5.</span> <span class="nav-text">SE Block （Sequeze and Excitation）</span></a></li></ol></li></ol></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhiqi Liu</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
