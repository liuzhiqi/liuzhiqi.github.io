<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.jpeg?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.jpeg?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.jpeg?v=5.1.4">


  <link rel="mask-icon" href="/images/favicon.jpeg?v=5.1.4" color="#222">





  <meta name="keywords" content=",,,">










<meta name="description" content="【Paper Reading】A Survey on Network Embedding2 分类与方法2.1 分类2.1.2 只考虑拓扑信息的网络 只考虑网络拓扑结构，相关的工作尝试保留网络的结构信息：如节点和边、邻居结构、高阶节点邻近度、社区结构等。  from nodes and links [10] to neighborhood structure [3], high-order pr">
<meta name="keywords" content="Social Network,Network Embedding,Network Analysis">
<meta property="og:type" content="website">
<meta property="og:title" content="【Paper Reading】A Survey on Network Embedding 0">
<meta property="og:url" content="https://liuzhiqi.github.io/draft/A_Survey_on_Network_Embedding.html">
<meta property="og:site_name" content="Zhiqi Liu">
<meta property="og:description" content="【Paper Reading】A Survey on Network Embedding2 分类与方法2.1 分类2.1.2 只考虑拓扑信息的网络 只考虑网络拓扑结构，相关的工作尝试保留网络的结构信息：如节点和边、邻居结构、高阶节点邻近度、社区结构等。  from nodes and links [10] to neighborhood structure [3], high-order pr">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2021-04-14T01:36:39.586Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="【Paper Reading】A Survey on Network Embedding 0">
<meta name="twitter:description" content="【Paper Reading】A Survey on Network Embedding2 分类与方法2.1 分类2.1.2 只考虑拓扑信息的网络 只考虑网络拓扑结构，相关的工作尝试保留网络的结构信息：如节点和边、邻居结构、高阶节点邻近度、社区结构等。  from nodes and links [10] to neighborhood structure [3], high-order pr">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://liuzhiqi.github.io/draft/A_Survey_on_Network_Embedding.html">





  <title>【Paper Reading】A Survey on Network Embedding 0 | Zhiqi Liu</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Zhiqi Liu</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-articles">
          <a href="/articles" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-book"></i> <br>
            
            Articles
          </a>
        </li>
      
        
        <li class="menu-item menu-item-notes">
          <a href="/categories/Notes" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-wpforms"></i> <br>
            
            Notes
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    
    
    
    <div class="post-block page">
      <header class="post-header">

	<h1 class="post-title" itemprop="name headline">【Paper Reading】A Survey on Network Embedding 0</h1>



</header>

      
      
      
      <div class="post-body">
        
        
          <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<h1 id="【Paper-Reading】A-Survey-on-Network-Embedding"><a href="#【Paper-Reading】A-Survey-on-Network-Embedding" class="headerlink" title="【Paper Reading】A Survey on Network Embedding"></a>【Paper Reading】A Survey on Network Embedding</h1><h2 id="2-分类与方法"><a href="#2-分类与方法" class="headerlink" title="2 分类与方法"></a>2 分类与方法</h2><h3 id="2-1-分类"><a href="#2-1-分类" class="headerlink" title="2.1 分类"></a>2.1 分类</h3><h4 id="2-1-2-只考虑拓扑信息的网络"><a href="#2-1-2-只考虑拓扑信息的网络" class="headerlink" title="2.1.2 只考虑拓扑信息的网络"></a>2.1.2 只考虑拓扑信息的网络</h4><ol>
<li>只考虑网络拓扑结构，相关的工作尝试保留网络的结构信息：如节点和边、邻居结构、高阶节点邻近度、社区结构等。<br>  from nodes and links [10] to neighborhood structure [3], high-order proximities of nodes [6], and community structures [4].</li>
<li>考虑网络结构属性，（如三角闭合性、结构平衡性质）<br> To name a few, network transitivity (i.e. triangle closure) is the driving force of link formation in networks [11], and structural balance property plays an important role in the evolution of signed networks [12].</li>
<li>考虑原始网络空间和embedding空间的统一性<br>  Some recent studies begin to look into this problem<br>and demonstrate the possibility of aligning these two spaces at the property level [8], [13].</li>
</ol>
<h4 id="2-1-3-带侧信息的NE"><a href="#2-1-3-带侧信息的NE" class="headerlink" title="2.1.3 带侧信息的NE"></a>2.1.3 带侧信息的NE</h4><p>除了网络的拓扑之外，一些网络还伴随着丰富的附加信息，如：节点内容和标签node content or labels in information networks [14]、节点和边的属性node and edge attributes in social networks [15]、异构网络中节点的类型node types in heteroge- neous networks [16]。<br>主要的挑战在于如何结合并平衡拓扑和附加信息在NE中的作用。Some multimodal and multisource fusion techniques are explored in this line of research [15], [17].</p>
<h4 id="2-1-4-有监督方法"><a href="#2-1-4-有监督方法" class="headerlink" title="2.1.4 有监督方法"></a>2.1.4 有监督方法</h4><p>之前两类方法学习网络表示通常使用无监督的方式，学得的embedding是通用的，可用于各种任务。而针对不同的目标问题可以进一步优化，通常采用有监督的方式。<br>Directly designing a framework of representation learning for a particular target scenario is also known as an end-to-end solution [18]<br>Some recent works demonstrate the feasibility in applications such as cascading prediction [18], anomaly detection [21], network alignment [22] and collaboration prediction [23].</p>
<h3 id="2-2-常用方法"><a href="#2-2-常用方法" class="headerlink" title="2.2 常用方法"></a>2.2 常用方法</h3><h4 id="2-2-1-矩阵分解"><a href="#2-2-1-矩阵分解" class="headerlink" title="2.2.1 矩阵分解"></a>2.2.1 矩阵分解</h4><p>NE的目标是得到低维向量空间以表示网络，这与矩阵分解方法具有相同的目标。常用的矩阵分解模型：奇异值分解（SVD）、非负矩阵分解</p>
<h4 id="2-2-2-随机游走"><a href="#2-2-2-随机游走" class="headerlink" title="2.2.2 随机游走"></a>2.2.2 随机游走</h4><p>类比于Word2Vec，基于随机游走的模型在网络中进行随机游走。将节点作为语言模型中的词，将随机游走作为句子，节点邻居可以用Word2Vec中共同出现的概率来定义。代表工作：DeepWalk（KDD 2014），Node2Vec（KDD 2016）</p>
<h4 id="2-2-3-深度神经网络"><a href="#2-2-3-深度神经网络" class="headerlink" title="2.2.3 深度神经网络"></a>2.2.3 深度神经网络</h4><p>作为非线性的学习模型，深度神经网络取得了很大的成功。代表性的使用深度神经网络的NE方法： SDNE [6], SDAE [26], and SiNE [13],</p>
<h2 id="3-Network-Embedding和Graph-Embedding对比"><a href="#3-Network-Embedding和Graph-Embedding对比" class="headerlink" title="3 Network Embedding和Graph Embedding对比"></a>3 Network Embedding和Graph Embedding对比</h2><p>Graph embedding（GE）的目标与network embedding的目标相似，是将一个图（graph）嵌入到低维的向量空间。传统的图嵌入方法，图是从以特征表示的数据集中构造得到的，如图像数据集。GE综述（Fu and Ma 2012）</p>
<h3 id="3-1-代表性的GE方法"><a href="#3-1-代表性的GE方法" class="headerlink" title="3.1 代表性的GE方法"></a>3.1 代表性的GE方法</h3><p>GE方法起初是作为一种降维的技术来被学习的。 一个图通常是由一些有特征的数据集构成的，如图像数据集。<br>作者主要提到了三种方法，Isomap，LLE，LE。均为经典流行学习的方法。这些方法均利用特征构建出图结构，并利用图结构行计算。</p>
<h4 id="Isomap-28-："><a href="#Isomap-28-：" class="headerlink" title="Isomap[28]："></a>Isomap[28]：</h4><p>Isomap 为经典流行学习方法，假设高维的数据都存在低维的本征结构。<br>Isomap利用流形在局部上与欧氏空间同胚这个性质（一个点的小邻域内，在流形空间的距离与实际空间中的距离近似），对每个点基于欧氏距离找出其近邻点，然后就能建立一个近邻连接图，图中近邻点之间存在连接，而非近邻点之间不存在连接，于是，计算两点之间测地线距离的问题就转变为计算近邻连接图上两点之间的最短路径问题.<br>Isomap通过使用连接线算法（如：knn）构建了一个邻居网络，从而构成流行空间，在近邻连接图上计算两点间的最短路径，从而构建距离矩阵。最终利用MDS的方法得到流行空间的表示(优化如下目标)。   </p>
<script type="math/tex; mode=display">
J(x') = \sum _i ^ N {|| (||x' _i - x' _j|| - d _{ij}} )|| \\\\</script><h4 id="LLE-局部线性嵌入"><a href="#LLE-局部线性嵌入" class="headerlink" title="LLE 局部线性嵌入."></a>LLE 局部线性嵌入.</h4><p>LLE同样使用连接线算法（如：knn）构建了一个邻居网络。Isomap构建网络目的是为了得到距离矩阵，而LLE构建网络则是希望能保证点与其紧邻的关系在变幻前后保持不变。<br>当构建好邻居网络之后，使用邻居节点来对自身点进行线性表示（ \( x _i = \sum _k ^K \omega _ {ij} x _j  \) ）。我们希望在进行流形变换之后的流形空间里，临近点同样保持相应的关系（ \( x’ _i = \sum _k ^K \omega _{ij} x’ _j  \), x’为变换后的x）。这一步，我们使用均方误差作为回归问题损失函数。  </p>
<script type="math/tex; mode=display">
\begin{cases}
J(\omega) = \sum _i ^ N {||x' _i - \sum _{x' _j \in G(x' _i)} \omega _{ij} x' _j || _2 ^2}  \\\\
\sum _{x _j \in G(x _i)} \omega _{ij} =1
\end{cases}</script><p>利用拉格朗日子乘法来求解这个最优化问题。<br>得到原始空间权重之后。我们反过来求解流形空间的点的表示。同样使用均方误差作为回归问题损失函数。       </p>
<script type="math/tex; mode=display">
J(x') = \sum _i ^ N {||x' _i - \sum _{x' _j \in G(x' _i)} \omega _{ij} x' _j || _2 ^2}  \\\\</script><h4 id="Laplacian-Eigenmaps-拉普拉斯特征映射"><a href="#Laplacian-Eigenmaps-拉普拉斯特征映射" class="headerlink" title="Laplacian Eigenmaps 拉普拉斯特征映射"></a>Laplacian Eigenmaps 拉普拉斯特征映射</h4><p>LE方法同样需要构建邻居网络，LE的目标是保证小邻域间的点的距离尽可能保持较小的状态。LE构建好邻居网络之后，利用邻居网络构建邻接矩阵：   </p>
<script type="math/tex; mode=display">
W_{ij} = e ^{-\frac{||x _i-x _j|| _2}{t}}</script><p>或直接利用邻接矩阵连通性：   </p>
<script type="math/tex; mode=display">
W _{ij} = \begin{cases}
1 \ if\ i\ and\ j\ is\ connected\\\\
0\ other
\end{cases}</script><p>通过优化，得到输出空间。</p>
<script type="math/tex; mode=display">
min\ arg _{x'} \sum _i \sum _j {||x' _i -x' _j|| ^2}W _{ij} =min\ arg _{X'} trace(X' ^TLX') \\\\
L = D-W \\\\
D _{ii} = \sum _{j=1} ^n W _{ij}</script><h3 id="3-2-Network-Embedding和Graph-Embedding区别"><a href="#3-2-Network-Embedding和Graph-Embedding区别" class="headerlink" title="3.2 Network Embedding和Graph Embedding区别"></a>3.2 Network Embedding和Graph Embedding区别</h3><p>区别主要存在于两个方面：目标与假设。<br>目标：<br>Network Embedding的目标在于：重建原始网络+网络推断。<br>Graph embedding主要目标是网络的重建。<br>因此，graph embedding 可以视为一种特殊的Network embedding，只考虑重建网络的network embedding。<br>GE更多的是处理数据特征特征的图结构，在原始特征空间中，图的权重可以被很好定义。而NE大多是处理真实的网络结构，例如社交网络，金融网络等。在这样的网络里，边之间的权重不好定义，可能需要特殊的分析与处理或特殊场景。<br>而现在的研究更注重网络的inference，因此NE也是本文接下来的重点。</p>
<h2 id="4-STRUCTURE-AND-PROPERTY-PRESERVING-NETWORK-EMBEDDING"><a href="#4-STRUCTURE-AND-PROPERTY-PRESERVING-NETWORK-EMBEDDING" class="headerlink" title="4 STRUCTURE AND PROPERTY PRESERVING NETWORK EMBEDDING"></a>4 STRUCTURE AND PROPERTY PRESERVING NETWORK EMBEDDING</h2><p>对于NE，最主要的目标就是保留网络结构和捕获网络特征。通常网络结构包括一阶网络特征和高阶网络特征。</p>
<h3 id="4-1-Structure-Preserving-Network-Embedding"><a href="#4-1-Structure-Preserving-Network-Embedding" class="headerlink" title="4.1 Structure Preserving Network Embedding"></a>4.1 Structure Preserving Network Embedding</h3><p>网络结构根据不同的力度可以分为不同类型。在NE方法中一般来说挖掘网络结构主要包括，邻居结构，高阶节点邻接特性和网络社群发现。   </p>
<h4 id="DeepWalk-3"><a href="#DeepWalk-3" class="headerlink" title="DeepWalk[3]"></a>DeepWalk[3]</h4><p>是早期影响力较大的图结构embedding方法。它主要是通过在图结构上随机游走，并将游走路径中节点生成序列。在随机游走过程中，利用权重生成转移概率：</p>
<script type="math/tex; mode=display">
P(v _j | v _i) =  \begin{cases}
\frac {M _ij}{\sum _{j \in {N _+(v _i)}} M _{ij}} v _ \in N _{+}(V _i)  \\\\
0\ e _{ij }\notin \epsilon
\end{cases}</script><p>其中M矩阵为权重矩阵，若无权重也可设为1.</p>
<h4 id="Node2vec-25"><a href="#Node2vec-25" class="headerlink" title="Node2vec[25]"></a>Node2vec[25]</h4><p>在DeepWalk基础上更进一步，他通过调整随机游走的权重的方法，使Embedding的结果可以倾向于同质性（距离相近的节点更相似）或结构性（结构相似的节点更加相似）。Node2vec 主要使用了权衡BFS 和DFS的方法，通过控制节点跳转概率达到调整的目的。其具体转移概率如下：  </p>
<script type="math/tex; mode=display">
\alpha _{pq}(t,x) = \begin{cases}
1/p ,if \ d _{tx} = 0（返回上一节点）\\\\
1 ,if \ d _{tx} = 1（保持1度关系）\\\\
1 ,if \ d _{tx} = 2（远离上一节点）
\end{cases}</script><script type="math/tex; mode=display">
P(x|v) = \alpha _{pq}(t,x)*\omega _{vx}</script><p>其中d 表示节点间距离，w表示权重，p为返回参数，p越小，随机游走回上一节点概率越大，q为进出参数，q越小，随机游走到远处概率越大。  </p>
<h4 id="LINE-10"><a href="#LINE-10" class="headerlink" title="LINE[10]"></a>LINE[10]</h4><p>可以应用于大规模网络中，其考虑网络的一阶和二阶相似性。一阶相似性是联合概率分布，由节点对之间的相似性来度量；二阶相似性是条件概率分布，通过节点生成其上下文节点的概率来度量；  </p>
<script type="math/tex; mode=display">
P1(v _i, v _2) = \frac {1}{1+exp(-u _i ^T u _j)}</script><script type="math/tex; mode=display">
P2(v _i | v _2) = \frac {exp(\overline u _j ^T \overline u _i)}{1+exp(-\overline u _i ^T \overline u _j)}</script><p>利用KL散度对最终结果进行优化（比较概率分布与边权重变化后得到的实际关系重要度）。得到一二阶特征后，可以通过特征拼接的方式组合简单组合在一起。  </p>
<h4 id="GraRep-【-Need-dive-deep-】："><a href="#GraRep-【-Need-dive-deep-】：" class="headerlink" title="GraRep 【## Need dive deep ##】："></a>GraRep 【## Need dive deep ##】：</h4><p>LINE只考虑一阶和二阶，GrapRep考虑K阶（K&gt;2）相似性。给定一个邻接矩阵A，k步概率转移矩阵可通过矩阵相乘计算得到。</p>
<h4 id="M-NMF"><a href="#M-NMF" class="headerlink" title="M-NMF:"></a>M-NMF:</h4><p>之前的NE主要从保留相近节点的关系来做Embedding。M-NMF则从社群划分的角度进行切入。<br>这里M-NMF优化目标可以分为两部分，矩阵相似度特征部分和社群部分。<br>矩阵相似度部分主要衡量了点和点的相似度。M-NMF主要使用了一阶相似度和二阶相似度。其中一阶相似度使用了邻接矩阵，作为相似度矩阵。二阶相似度矩阵则由两点的邻接向量的余弦距离定义。最终相似度矩阵由一阶二阶相似度矩阵加权拼接得到：\(  S = S ^{(1)} + \eta S ^{(2)}  \)    </p>
<p>社群部分主要使用了Q-Modularity来衡量社群划分水平（见：基础社群分析<br>）。通过优化：\( Tr(H ^TBH) \)来优化社群。<br>为了得到Embedding向量，M-NMF使用了矩阵分解，目标函数如下：</p>
<script type="math/tex; mode=display">
argmin(M,U,H,C)||S - MU ^T|| ^2 _F + \alpha||H - UC ^T|| ^2 _F -\beta Tr(H ^TBH)</script><p>其中U为节点表示矩阵，M为节点-节点偏好矩阵，C为社区特征矩阵。铜鼓轮流更新M、U、H、C来优化方程。</p>
<p>从公式中我们可以看到，作者通过引入一个新的“社区表达矩阵”C，结合“节点表达矩阵”U，通过矩阵分解的方法来结合模块度公式中原有的“社区矩阵”H，这样一来，embedding的过程中使用模块度公式来约束社区结构。但是这种方法引入的假设变量有点多，虽然实验效果不错，但解释起来始终有些牵强。同时，矩阵分解方法需要开的内存太高，难以针对大规模数据。</p>
<h4 id="SDNE"><a href="#SDNE" class="headerlink" title="SDNE"></a>SDNE</h4><p>SDNE可以看作是基于LINE的扩展，同时也是第一个将深度学习应用于网络表示学习中的方法。SDNE解决高非线性、结构保护和稀疏性问题，SDNE深度自编码器的基础上，优化了自编码器损失函数，同时对表示层也加入了有监督的损失函数。<br>传统自编码器损失函数：  </p>
<script type="math/tex; mode=display">
L = \sum _{i=1} ^n||\hat x _i - x _i|| _2 ^2</script><p>我们只能根据已有的连接表示节点之间的相似性,但对它们的差异无法作出比较好的衡量。<br>而且由于网络稀疏,输入实例中0元素的数目远多于非0元素。因此SDNE对损失函数加入了更多罚项：</p>
<script type="math/tex; mode=display">
L = \sum _{i=1} ^n||(\hat x _i - x _i)\cdot b _i|| _2 ^2</script><p>其中bi 由邻接矩阵得到，若i j对应到邻接矩阵等于0，则 \( b _{ij} = 1 \) 。否则\( b _{ij}=\beta &gt;1 \) 。通过自编码器,如果两个节点具有相近的邻接点结构,则在表示空间中距离越近（因为损失函数加入了更多邻接信息罚项）。<br>另一方面，对于表示层，SDNE加入了损失函数：  </p>
<script type="math/tex; mode=display">
L = \sum _{i,j=1} ^n s _{i,j} ||y _i ^{(K)}-y _j ^{(K)} ||</script><p>其中 \(  y _i ^{(K)} \)为自编码器得到的编码层，s为邻接矩阵。从而在表示层加入了一阶邻接信息。同时为了优化算法，在表示层的学习中加入了Laplacian Eigenmaps的思想，通过构建相似关系图来重构局部特征结构，从而使局部小临域内点更接近。</p>
<h4 id="Cao-2017"><a href="#Cao-2017" class="headerlink" title="Cao 2017"></a>Cao 2017</h4><p>对于Deep Walk模型而言，其长度有限，处于边缘的节点信息没有用全，且参数难调。因此Cao提出了一个深度学习的模型来学习图的顶点表示，借鉴了PageRank，结合加权转移概率矩阵，先用一个随机搜索模型来获取图的结构信息，生成一个共现概率矩阵，和DeepWalk相比是省去了一个采样的过程，然后基于共现概率矩阵计算PPMI矩阵，PPMI矩阵可以看成是稀疏的顶点高维表示，再用堆叠自动编码器从PPMI矩阵中学习到顶点的低维表示。</p>
<h4 id="GEM-D-2017"><a href="#GEM-D-2017" class="headerlink" title="GEM-D 2017"></a>GEM-D 2017</h4><p>提出一个NE框架，统一之前的算法。包括三个度量 [h(⋅),g(⋅),d(⋅,⋅)]。 分别为邻近度函数，非线性函数，度量h和g差异性的度量函数。</p>
<h3 id="4-2-Property-Preserving-Network-Embedding"><a href="#4-2-Property-Preserving-Network-Embedding" class="headerlink" title="4.2 Property Preserving Network Embedding"></a>4.2 Property Preserving Network Embedding</h3><p>上文提到的方法，大多focus在网络的结构或者节点间的关系身上，实际上除了结构之外，我们同样也关注节点或边自身的性质。</p>
<h4 id="Multi-Component-Hashing-Ou-et-al-2015-44"><a href="#Multi-Component-Hashing-Ou-et-al-2015-44" class="headerlink" title="Multi-Component Hashing [Ou et al 2015.[44]]"></a>Multi-Component Hashing [Ou et al 2015.[44]]</h4><p>MuCH的核心是解决网络结构中关系非传递性问题（A与B存在关系，B与C存在关系，A与C不存在关系）。<br>MuCH使用了投影矩阵，通过构建M个投影矩阵\( \{W _i\} ^M \)，将每个个体重新用M个Hash向量表示，在进行目标优化时，使用了实体间真实相似性。这样做的好处是一方面使用投影矩阵压缩了特征表示，同时，也使表示能反应真实的相似性，最后得到的哈希表示，也使目标查询更加方便与快速。（PS：感觉似乎就是一个简单的神经网络，使用一层分M块参数矩阵 \( \{W _i\} ^M \)，激活函数使用sgn，输出使用softmax）</p>
<h4 id="Max-Margin-DeepWalk-Asymmetric-Transitivity-Preserving-Graph-Embedding-HOPE-2016-8"><a href="#Max-Margin-DeepWalk-Asymmetric-Transitivity-Preserving-Graph-Embedding-HOPE-2016-8" class="headerlink" title="Max-Margin DeepWalk: Asymmetric Transitivity Preserving Graph Embedding [HOPE 2016 [8]]"></a>Max-Margin DeepWalk: Asymmetric Transitivity Preserving Graph Embedding [HOPE 2016 [8]]</h4><p>考虑有向网络的非对称传递性（A-&gt;B B-&gt;C 则有 A-&gt;C （而不是C-&gt;A））。对于每个顶点，该方法的到两个embedding后的向量，一个是source，一个是target。因为是有向图，此顶点可能是一条路径的源头，也有可能是一条路径的终点。方法的目标是优化：</p>
<script type="math/tex; mode=display">
min||S - U ^s {U ^t} ^T|| _F ^2</script><p>其中S为相似矩阵，因为是有向图，所以其\( S _{ij} \)表示节点i作为source顶点与节点j作为target顶点的相似度。两个不同的U矩阵分别代表顶点作为source和target的Embedding向量矩阵。</p>
<p>该方法实际上是希望找到节点作为source和targe的不同embedding表示，同时该表示能还原节点的相似度。即希望：</p>
<script type="math/tex; mode=display">
S = \sum _{i} ^N \sigma _i v _i ^s {v _i ^t} ^T \\\\
U ^s = [\sqrt{\sigma _1}v _1 ^s,\dots,{\sigma _K}v _K ^s] \\\\
U ^t = [\sqrt{\sigma _1}v _1 ^t,\dots,{\sigma _K}v _K ^t] \\\\
S = V ^s \Sigma V ^t</script><p>即希望S可通过SVD得到U。</p>
<p>为了度量S相似性矩阵，HOPE总结了4种度量方法：Katz Index [45], Rooted PageRank [7], Common Neighbors [7], and Adamic-Adar [46]，相关内容可见《<a href>网络节点距离度量</a>》。  </p>
<p>由于先求S在对S进行SVD对大数据不太友好，因此作者使用了以下方法：</p>
<h5 id="Katz-Index"><a href="#Katz-Index" class="headerlink" title="Katz Index"></a>Katz Index</h5><script type="math/tex; mode=display">
M _g = (I -\beta A)   \\\\
M _l =   \beta A</script><h5 id="Rooted-PageRank-RPR"><a href="#Rooted-PageRank-RPR" class="headerlink" title="Rooted PageRank (RPR)"></a>Rooted PageRank (RPR)</h5><script type="math/tex; mode=display">
M _g = (1-\alpha P)   \\\\
M _l =   (1-\alpha) I</script><h5 id="Rooted-PageRank-RPR-1"><a href="#Rooted-PageRank-RPR-1" class="headerlink" title="Rooted PageRank (RPR)"></a>Rooted PageRank (RPR)</h5><script type="math/tex; mode=display">
M _g = I  \\\\
M _l = A ^2</script><h5 id="Adamic-Adar-AA）"><a href="#Adamic-Adar-AA）" class="headerlink" title="Adamic-Adar (AA）"></a>Adamic-Adar (AA）</h5><script type="math/tex; mode=display">
M _g = I  \\\\
M _l = ADA</script><p>因此s的svd变成了：</p>
<script type="math/tex; mode=display">
M _g ^{-1}M _l = V ^s \Sigma{V ^t} ^T \\\\
 = ADA</script><p>将原始SVD转变为通用的SVD问题，也就是可以不求s直接进行SVD。</p>
<h4 id="SiNE（2017）"><a href="#SiNE（2017）" class="headerlink" title="SiNE（2017）"></a>SiNE（2017）</h4><hr>
<p><a href="https://blog.csdn.net/travalscx/article/details/86670523" target="_blank" rel="noopener">论文笔记：Asymmetric Transitivity Preserving Graph Embedding</a></p>

        
      </div>
      
      
      
    </div>
    
    
    
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpeg" alt="Zhiqi Liu">
            
              <p class="site-author-name" itemprop="name">Zhiqi Liu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/liuzhiqi" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:yourname@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#【Paper-Reading】A-Survey-on-Network-Embedding"><span class="nav-number">1.</span> <span class="nav-text">【Paper Reading】A Survey on Network Embedding</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-分类与方法"><span class="nav-number">1.1.</span> <span class="nav-text">2 分类与方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-分类"><span class="nav-number">1.1.1.</span> <span class="nav-text">2.1 分类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-只考虑拓扑信息的网络"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">2.1.2 只考虑拓扑信息的网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-3-带侧信息的NE"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">2.1.3 带侧信息的NE</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-4-有监督方法"><span class="nav-number">1.1.1.3.</span> <span class="nav-text">2.1.4 有监督方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-常用方法"><span class="nav-number">1.1.2.</span> <span class="nav-text">2.2 常用方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-矩阵分解"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">2.2.1 矩阵分解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-2-随机游走"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">2.2.2 随机游走</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-3-深度神经网络"><span class="nav-number">1.1.2.3.</span> <span class="nav-text">2.2.3 深度神经网络</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Network-Embedding和Graph-Embedding对比"><span class="nav-number">1.2.</span> <span class="nav-text">3 Network Embedding和Graph Embedding对比</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-代表性的GE方法"><span class="nav-number">1.2.1.</span> <span class="nav-text">3.1 代表性的GE方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Isomap-28-："><span class="nav-number">1.2.1.1.</span> <span class="nav-text">Isomap[28]：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LLE-局部线性嵌入"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">LLE 局部线性嵌入.</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Laplacian-Eigenmaps-拉普拉斯特征映射"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">Laplacian Eigenmaps 拉普拉斯特征映射</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Network-Embedding和Graph-Embedding区别"><span class="nav-number">1.2.2.</span> <span class="nav-text">3.2 Network Embedding和Graph Embedding区别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-STRUCTURE-AND-PROPERTY-PRESERVING-NETWORK-EMBEDDING"><span class="nav-number">1.3.</span> <span class="nav-text">4 STRUCTURE AND PROPERTY PRESERVING NETWORK EMBEDDING</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Structure-Preserving-Network-Embedding"><span class="nav-number">1.3.1.</span> <span class="nav-text">4.1 Structure Preserving Network Embedding</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DeepWalk-3"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">DeepWalk[3]</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Node2vec-25"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">Node2vec[25]</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LINE-10"><span class="nav-number">1.3.1.3.</span> <span class="nav-text">LINE[10]</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GraRep-【-Need-dive-deep-】："><span class="nav-number">1.3.1.4.</span> <span class="nav-text">GraRep 【## Need dive deep ##】：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#M-NMF"><span class="nav-number">1.3.1.5.</span> <span class="nav-text">M-NMF:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SDNE"><span class="nav-number">1.3.1.6.</span> <span class="nav-text">SDNE</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Cao-2017"><span class="nav-number">1.3.1.7.</span> <span class="nav-text">Cao 2017</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GEM-D-2017"><span class="nav-number">1.3.1.8.</span> <span class="nav-text">GEM-D 2017</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Property-Preserving-Network-Embedding"><span class="nav-number">1.3.2.</span> <span class="nav-text">4.2 Property Preserving Network Embedding</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Multi-Component-Hashing-Ou-et-al-2015-44"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">Multi-Component Hashing [Ou et al 2015.[44]]</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Max-Margin-DeepWalk-Asymmetric-Transitivity-Preserving-Graph-Embedding-HOPE-2016-8"><span class="nav-number">1.3.2.2.</span> <span class="nav-text">Max-Margin DeepWalk: Asymmetric Transitivity Preserving Graph Embedding [HOPE 2016 [8]]</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Katz-Index"><span class="nav-number">1.3.2.2.1.</span> <span class="nav-text">Katz Index</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Rooted-PageRank-RPR"><span class="nav-number">1.3.2.2.2.</span> <span class="nav-text">Rooted PageRank (RPR)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Rooted-PageRank-RPR-1"><span class="nav-number">1.3.2.2.3.</span> <span class="nav-text">Rooted PageRank (RPR)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Adamic-Adar-AA）"><span class="nav-number">1.3.2.2.4.</span> <span class="nav-text">Adamic-Adar (AA）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SiNE（2017）"><span class="nav-number">1.3.2.3.</span> <span class="nav-text">SiNE（2017）</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhiqi Liu</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
