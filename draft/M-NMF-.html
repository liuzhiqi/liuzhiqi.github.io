<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.jpeg?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.jpeg?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.jpeg?v=5.1.4">


  <link rel="mask-icon" href="/images/favicon.jpeg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="\( \)$$ $$ \begin{cases} \end{cases}!bubble RNNSearch[45]  3 Attention Mechanisms从2017年中，更多的研究开始关注与End-to-end的attention模型，如Neural Transformer [37]与Graph Attention Networks (GATs)[97]。与之前仅将attentio">
<meta property="og:type" content="website">
<meta property="og:title" content="Zhiqi Liu">
<meta property="og:url" content="https://liuzhiqi.github.io/draft/M-NMF-.html">
<meta property="og:site_name" content="Zhiqi Liu">
<meta property="og:description" content="\( \)$$ $$ \begin{cases} \end{cases}!bubble RNNSearch[45]  3 Attention Mechanisms从2017年中，更多的研究开始关注与End-to-end的attention模型，如Neural Transformer [37]与Graph Attention Networks (GATs)[97]。与之前仅将attentio">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://liuzhiqi.github.io/draft/Transformer.jpg">
<meta property="og:image" content="https://liuzhiqi.github.io/draft/Transformer_evo.png">
<meta property="og:image" content="https://liuzhiqi.github.io/draft/Star-transformer.jpg">
<meta property="og:image" content="https://liuzhiqi.github.io/draft/Sparse_Transformers.png">
<meta property="og:image" content="https://liuzhiqi.github.io/draft/Input-Combination-Strategies-for-Multi-Source-Transformer-Decoder.png">
<meta property="og:image" content="https://liuzhiqi.github.io/draft/HighWay_Recurrent_Transformer.png">
<meta property="og:image" content="https://liuzhiqi.github.io/draft/TTS_Transformer.png">
<meta property="og:image" content="https://liuzhiqi.github.io/draft/BERT1.png">
<meta property="og:image" content="https://liuzhiqi.github.io/draft/top-down_bottom-up.png">
<meta property="og:updated_time" content="2021-06-03T12:06:36.734Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Zhiqi Liu">
<meta name="twitter:description" content="\( \)$$ $$ \begin{cases} \end{cases}!bubble RNNSearch[45]  3 Attention Mechanisms从2017年中，更多的研究开始关注与End-to-end的attention模型，如Neural Transformer [37]与Graph Attention Networks (GATs)[97]。与之前仅将attentio">
<meta name="twitter:image" content="https://liuzhiqi.github.io/draft/Transformer.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://liuzhiqi.github.io/draft/M-NMF-.html">





  <title> | Zhiqi Liu</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Zhiqi Liu</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-articles">
          <a href="/articles" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-book"></i> <br>
            
            Articles
          </a>
        </li>
      
        
        <li class="menu-item menu-item-notes">
          <a href="/categories/Notes" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-wpforms"></i> <br>
            
            Notes
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    
    
    
    <div class="post-block page">
      <header class="post-header">

	<h1 class="post-title" itemprop="name headline"></h1>



</header>

      
      
      
      <div class="post-body">
        
        
          <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>



<p>\( \)<br>$$</p>
<p>$$</p>
<p>\begin{cases}</p>
<p>\end{cases}<br>!<a href="./attention_survey/1.png">bubble</a></p>
<p><a href="#RNNSearch">RNNSearch[45]</a><br><a name="Q67"> </a></p>
<h1 id="3-Attention-Mechanisms"><a href="#3-Attention-Mechanisms" class="headerlink" title="3 Attention Mechanisms"></a>3 Attention Mechanisms</h1><p>从2017年中，更多的研究开始关注与End-to-end的attention模型，如<a href="#Transformer">Neural Transformer [37]</a>与<a href="#GATs">Graph Attention Networks (GATs)[97]</a>。与之前仅将attention作为工具加入网络的一部分不同，这两种方法都是纯attention框架，这为attention成为深度学习中的重要元素提供了一些理论保证。</p>
<h3 id="2-4-1-Transformer-Based"><a href="#2-4-1-Transformer-Based" class="headerlink" title="2.4.1 Transformer Based"></a>2.4.1 Transformer Based</h3><p><a name="Transformer"> </a><br>首先我们来介绍一下基础的Transformer，Transformer源于Google2017的文章：<strong>Attention is all you need</strong><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">[Google2017, 37]</a>。作者提出了Neural Transformer的模型，Neural Transformer是第一个仅使用attention和全联接神将网络来学习序列数据的模型。其主要解决的也是机器翻译问题，和其他翻译模型一样，Transformer也使用了Encoder-Decoder框架，其具体架构如下：<br><img src="//liuzhiqi.github.io/draft/Transformer.jpg" alt="bubble"><br>我们从下而上从左到右开始分析：<br><strong>Positional Encoding</strong>：对于Transformer模型，由于输入是同时送入网络，所以丢失了序列的位置信息，因此对于模型输入需要选择好的位置信息的表达，并融合进输入。对于好的Positional Encoding首先其值不能发散，如用0、1、2这样的位置信息编码，过大的值的位置会导致学习时产生数值倾斜。同时，像Transformer这样使用位置信息叠加到Embedding结果这种潜入方式，过大的值容易掩埋Embedding信息。其次，好的位置编码最好不能受到文本长度的影响，如除以文本长度均匀映射到[0,1]之间这种做法，对于不同长度的文本，编码后距离相同的两个词，短文本的真正间隔可能要远小于长文本，这样会导致位置编码的相对次序缺失。具体位置编码规则如下：  </p>
<p>$$<br>PE(pos,2i)=sin(\frac {pos}{10000 ^{2i/d _{model}}}) \\<br>PE(pos,2i+1)=cos(\frac {pos}{10000 ^{2i/d _{model}}}) \\<br>$$<br>可以看出，这里作者是将位置信息在频域做了近似分解，而选择这种分解方式的好处是PE(pos+k, 2i)可表示为PE(pos, 2i)的线形表示（根据三角函数公式）：<br>$$<br>PE(pos+k,2i)= PE(pos,2i)\times PE(k,2i+1)+PE(pos,2i+1)\times PE(k,2i)\\<br>PE(pos+k,2i+1)= PE(pos,2i+1)\times PE(k,2i+1)-PE(pos,2i)\times PE(k,2i)\\<br>$$<br>因为PE(k, 2i+1)是常量, 所以就有固定的系数. 也就是某种意义上的相对位置编码.    </p>
<p><strong>Multi-Head Attention</strong>：首先Transformer模型使用了Scaled Dot-Product Attention，即使用内积计算query向量与每一个key的相关性，并将其Softmax做为权重，为了方便表示，我们将query向量复制表示成如下矩阵形式：<br>$$<br>Attention(Q,K,V)=softmax(\frac {QK ^T}{\sqrt{d _k}})V \\<br>Q = [q ,\dots,q ]^T<br>$$<br>注意这里除以d的开方，是因为内积之后方差扩大了d倍，为了弥补这一问题，因此除以方差倍数（个人感觉其原理类似Normalization）<br>其次，Attention为Self-Attention，即Q=K=V。<br>为了学习更多表达，作者引入了Multi-Head Attention，即将Q、K、V投影到不同的潜空间，希望能用这种方式学到数据更多方面的表示：<br>$$<br>MutiHead(Q,K,V)=Concat(head _1,\dots,head _h)W ^O\\<br>head _i= Attention(QW _i ^Q,KW _i ^K,VW _i ^V)<br>$$<br>这里作者设置h=8，各W矩阵将原向量投影到d/h=64维，通过降维投影来表达不同方面，最后将各个表达加权聚合。同时值得注意的是，虽然使用self-attention，QKV实际意义是相同的，但不能使用相同的投影矩阵，尤其是QK的参数矩阵，因为如果使用相同参数矩阵，则attention进行相关性计算时，得到的相关性矩阵会成为对称矩阵，实际上对query和key来说，(词1在query，词2在Key)与(词2在query，词1在Key)这两个重要性得分应该是不同的，因此共享参数矩阵会降低表达。</p>
<p><strong>Add&amp;Norm</strong>：从框架图中也可看出，Transformer使用了残差网络，同时加入了LayerNorm，即输出为LayerNorm(x+sublayer(x))。这里使用LayerNorm主要是为了将中间层输出分布还原回均值为0方差为1的分布，从而降低ICS，防止梯度消失等问题。更多讨论如为什么使用LN而不是BN（<a href="https://www.zhihu.com/question/395811291" target="_blank" rel="noopener">讨论</a>）等，可以参考Transformer与Normalization相关文章。</p>
<p><strong>Feed-Forward</strong>：在进行了Attention操作之后，encoder和decoder中的每一层都包含了一个全连接前向网络，对每个position的向量分别进行相同的操作，包括两个线性变换和一个ReLU激活输出。<br>$$<br>FFN(x)=max(0,xW _1 +b _1)W _2 +b _2<br>$$</p>
<p><strong>Masked Multi-Head Attention</strong>：序列模型在decoder部分加入Masked，更多程度上是一种工程上的保证，主要是为了增加模型并行程度。在训练时，首先训练数据往往是batch输入并以长度相同向量表示，而对于句子由于句长不同，因此短句需要补齐mask padding。同时在训练位置T时，T后的数据属于需要预测的部分，因此不能加入attention，所以用mask将其掩盖。Mask主要是通过乘以Mask矩阵，将多余部分替换成一个很大的负值，从而使其attention权重为0。这样的操作更多是工程上的考虑，当然也可以在前向后向训练中加入逻辑判断，不过这样code会变得复杂。</p>
<p>基于Transformer模型，有很多改进方法，如下图：<br><img src="//liuzhiqi.github.io/draft/Transformer_evo.png" alt="bubble"><br>接下来我们根据年份顺序对各种改进进行简单讲解。</p>
<h4 id="2-4-1-1-Transformer方法的改进"><a href="#2-4-1-1-Transformer方法的改进" class="headerlink" title="2.4.1.1 Transformer方法的改进"></a>2.4.1.1 Transformer方法的改进</h4><h5 id="Weighted-Transformer-：Weighted-transformer-network-for-machine-translation-2017-102"><a href="#Weighted-Transformer-：Weighted-transformer-network-for-machine-translation-2017-102" class="headerlink" title="Weighted Transformer ：Weighted transformer network for machine translation[2017, 102])"></a>Weighted Transformer ：Weighted transformer network for machine translation<a href="https://arxiv.org/pdf/1711.02132.pdf" target="_blank" rel="noopener">[2017, 102]</a>)</h5><p>在基础Transformer上对muti-attention进行了调整对每一个head加入了权重。Base Transformer认为，每个head是平权的，因此直接concat所有header然后使用投影矩阵W^O，投影到表示空间。而Weighted Transformer认为每一个head有各自的表达，有各自的权重，因此每一个head，各自直接投影到表示空间然后乘以head权重k，然后各自输入到FFN进行学习，最终学出的结果乘以权重alpha累加合并到最终表示空间，从而在FFN后融合多个Branch 的head。可以看出Weighted Transformer 实际上是形成了多个Branch，权重k和alpha均是本文新增的要学习的参数，这里k的和与alpha的和均为1。</p>
<p><a name="Star-transformer"> </a></p>
<h5 id="Star-transformer-NAACL2019-103"><a href="#Star-transformer-NAACL2019-103" class="headerlink" title="Star-transformer[NAACL2019, 103])"></a>Star-transformer<a href="https://arxiv.org/pdf/1902.09113.pdf" target="_blank" rel="noopener">[NAACL2019, 103]</a>)</h5><p>对输入长度为N的序列，transformer使用了全联接的attention这样做一方面使得参数量过大（N^2的参数量），训练需要大量样本，另一方面，若序列存在局部相关的先验知识时，对于局部相关的先验几乎没有利用。为此Star-transformer简化了transformer全联接的模型，采用了星型的连接结构，使训练既能捕捉局部相关先验，又能捕获全局或者说长期信息。其具体改进过程如下：<br><img src="//liuzhiqi.github.io/draft/Star-transformer.jpg" alt="bubble"><br>其中外围节点h对应序列每一项，h只与其前后项和全局项s有关，因此在更新h时，只需要对上一轮前后两项、上一轮当前项、上一轮全局项进行attention即可，同时可以看到attention还加入了输入项e，这里可以看作保留了原transformer残差网络的思路。对于全局项s，在更新完全部h后，使用attention融合上一轮s和全部h。可以看到，在h和s的更新时，作者去掉了transformer输出时的两层全联接网络，仅使用ReLU，从而对网络进行了再次简化。<br>Star-transformer最终输出的是s和H，可以将其输入MLP进行分类或作为特征输出到其他任务网络。</p>
<h5 id="Self-Attention-with-Relative-Position-Representations-NAACL2018-174"><a href="#Self-Attention-with-Relative-Position-Representations-NAACL2018-174" class="headerlink" title="Self-Attention with Relative Position Representations[NAACL2018, 174])"></a>Self-Attention with Relative Position Representations<a href="https://arxiv.org/pdf/1803.02155.pdf" target="_blank" rel="noopener">[NAACL2018, 174]</a>)</h5><p>基础transformer对于序列每一项的位置信息仅在输入时使用正弦位置编码来提供，这种方式编码了位置的绝对信息，而本文作者提出一个self-attention的扩展来考虑元素之间的成对关系。对于序列每一项，作者构建了一个全联接图，且令\(a _{ij} ^v,a _{ij} ^k \in R ^{d _a}\)为i和j之间的边（作者将key空间和value空间的边分开进行了表示）。则Attention公式融合边的信息修改为：   </p>
<p>$$<br>clip(x,k) = max(-k,min(k,x)) \\<br>a _{ij} ^K = w _{clip(j-i,k)} ^K \\<br>a _{ij} ^V = w _{clip(j-i,k)} ^V \\<br>e _{ij} = \frac {(x _i W ^Q)(x _jW ^K + a _{ij} ^K) ^T}{\sqrt {d _z}}\\<br>z _i = \sum _{j=1} ^n {\alpha _{ij}(x _j W ^V + \alpha _{ij} ^v)}<br>$$</p>
<p>首先对于序列的成对关系，作者给出了限制，其认为只有距离为k的点对值得考虑，超出k则两点间影响可以固定（既clip函数裁剪下标）。从而构成了从-k到k一共2k+1个相对位置表示，将这些表示作为要学习的参数，按上式融入attention中，最终学到不同空间固定的位置表示。同时文中提到，问了减少学习参数，每个head的位置表示可以共享参数。</p>
<h5 id="Sparse-Transformers-：Generating-Long-Sequences-with-Sparse-Transformers-OpenAl2019-103"><a href="#Sparse-Transformers-：Generating-Long-Sequences-with-Sparse-Transformers-OpenAl2019-103" class="headerlink" title="Sparse Transformers ：Generating Long Sequences with Sparse Transformers[OpenAl2019, 103])"></a>Sparse Transformers ：Generating Long Sequences with Sparse Transformers<a href="https://arxiv.org/pdf/1904.10509.pdf" target="_blank" rel="noopener">[OpenAl2019, 103]</a>)</h5><p>当序列过长时，基础Transformers的attention计算将非常耗时，为了降低计算量和存储， Sparse Transformers从局部相关和全局相关两个角度来进行attention，对于局部相关，Sparse Transformers设计了一个范围k，对前后相邻k个元素进行attention。对于全局信息，Sparse Transformers则参考膨胀卷积的思路，隔l位进行抽样，因为attention迭代了很多轮，所以高层attention会逐渐融合全局信息。最终attention关注位置如下（蓝色部分）：</p>
<p><img src="//liuzhiqi.github.io/draft/Sparse_Transformers.png" alt="bubble"><br>从而极大的减少了计算时间和存储，使很长的输入序列也有很好的效果。</p>
<h5 id="Set-transformer-A-framework-for-attention-based-permutation-invariant-neural-networks-ICML2019-106"><a href="#Set-transformer-A-framework-for-attention-based-permutation-invariant-neural-networks-ICML2019-106" class="headerlink" title="Set transformer: A framework for attention-based permutation-invariant neural networks.[ICML2019, 106])"></a>Set transformer: A framework for attention-based permutation-invariant neural networks.<a href="http://proceedings.mlr.press/v97/lee19d/lee19d.pdf" target="_blank" rel="noopener">[ICML2019, 106]</a>)</h5><p>针对输入不是序列而是set（顺序无关），作者提出了一种另一种减少self-attention计算复杂度的思路。其中，有趣的点是设计了Inducing point。传统Attention输入一般为Q、K、V三部分，而这三部分往往来自于真实数据或特征，本文设计了Inducing point，即将query向量变成参数，从而学出要查询的特征。通俗来讲，例如：想利用attention从众多图片中识别猫，则Inducing query可能就会学出猫的特征。这一思路扩展了attention应用的方式，很有借鉴意义。</p>
<h4 id="2-4-1-2-Transformer方法的应用"><a href="#2-4-1-2-Transformer方法的应用" class="headerlink" title="2.4.1.2 Transformer方法的应用"></a>2.4.1.2 Transformer方法的应用</h4><h5 id="Doubly-attentive-transformer-machine-translation-2018-107"><a href="#Doubly-attentive-transformer-machine-translation-2018-107" class="headerlink" title="Doubly attentive transformer machine translation.[2018, 107])"></a>Doubly attentive transformer machine translation.<a href="https://arxiv.org/pdf/1807.11605.pdf" target="_blank" rel="noopener">[2018, 107]</a>)</h5><p>对Transformer的简单推广，将输入扩展到图文混合，图片用CNN抽取特征，文本仍用Transformer encoder 对与decoder attention部分使用文本和图片特征一起做attention。</p>
<h5 id="Input-Combination-Strategies-for-Multi-Source-Transformer-Decoder-2018-106"><a href="#Input-Combination-Strategies-for-Multi-Source-Transformer-Decoder-2018-106" class="headerlink" title="Input Combination Strategies for Multi-Source Transformer Decoder[2018, 106])"></a>Input Combination Strategies for Multi-Source Transformer Decoder<a href="https://www.aclweb.org/anthology/W18-6326.pdf" target="_blank" rel="noopener">[2018, 106]</a>)</h5><p>总结了Transformer融合多模型特征的方法，主要分成四种：serial、parallel、flat、hierarchical。serial为顺序融合，即依次对不同特征进行attention，结果作为下一个Attention query。parallel是各自做attention然后叠加到一起。flat使用concat叠加特征KV向量，然后一起做attention。Hierarchical则个字attention，然后再用attention对每个模型输出汇总。以下为不同方法试验结果。</p>
<p><img src="//liuzhiqi.github.io/draft/Input-Combination-Strategies-for-Multi-Source-Transformer-Decoder.png" alt="bubble">  </p>
<h5 id="Input-Combination-Strategies-for-Multi-Source-Transformer-Decoder-2019-106"><a href="#Input-Combination-Strategies-for-Multi-Source-Transformer-Decoder-2019-106" class="headerlink" title="Input Combination Strategies for Multi-Source Transformer Decoder[2019, 106])"></a>Input Combination Strategies for Multi-Source Transformer Decoder<a href="https://www.aclweb.org/anthology/P19-1601.pdf" target="_blank" rel="noopener">[2019, 106]</a>)</h5><p>充分利用Transformer的encoder和decoder结构，来解决文本风格切换的问题。对于输入假设有k种风格，则对于每个风格有各自的语料库，由于不同风格之间文本缺少对应关系，因此作者对Transformer进行改进设计了一种类似GAN的结构。作者将风格迁移这个任务进行分解，其实他有三个子任务，一是语言本身，即语言流利； 二是语义的保留；三是风格的迁移。      </p>
<ol>
<li>对于语言本身的流利，采用了原文句子输入到输入的auto encoder-decoder的方式，即对输入编码，再解码，loss 则是生成句子和原来句子的差别的交叉熵。区别于其他的工作，这里采用了transformer, 其本身就是堆叠的自编码器和解码器。  </li>
<li>语义的保留，作者采用的方式是，对于 一个句子x，加上风格s’ , 编码后的结果fe( x , s’)作为输入，基于原风格s，进行重建，生成x。    </li>
<li>对于子任务3，则是借助一个判别网络 , 它可以判断输入fe( x , s’) 的风格 s。 对于这个判别网络的训练，作者采用了两个loss， 一个是条件判别的loss，这个类似于GAN ，其实就是对于原始输入x ，和基于原风格生成的fe( x , s) 输出正标签，而对于其他风格的生成fe( x , s’)输出负。 另一种多标签判别的训练，则是输出语句的风格标签，共K+1个（伪生成语句的标签是0）。</li>
</ol>
<p>#####Hierarchical Transformer: Hierarchical transformers for multi-document summarization<a href="https://www.aclweb.org/anthology/P19-1601.pdf" target="_blank" rel="noopener">[ACL2019, 110]</a>)<br>针对多文本摘要问题，有由于输入是多个文档，因此对于端到端模型而言，输入太过庞大。本文通过先用无监督评估标题和文本段落匹配度评分（LSTM编码标题和段落并各自pooling压缩后计算匹配得分）选出top K的段落。然后输入改进的Transformer模型，对于Transformer模型作者Encoding加入了层次Encoding方法，即现对段落内部进行MH-Attention得到表示，再使用Multi-head Pooling（类似muti-head attention，投影到不同空间后计算attention，将一整段词汇融合成多头段落表示）得到段落多头表示，再利用attention计算融合多个段落，得到新的段落表达，并将段落表达融合进段落的每一个词中（同一段内融合的段落表达是相同的），其余部分和普通attention一样。作者利用这种分层的attention，即解决了多个文档输入过长的问题，同时也是每一个词都融合了各自段落或各自文档的信息。</p>
<h5 id="HighWay-Recurrent-Transformer：Learning-multi-level-information-for-dialogue-response-selection-by-highway-recurrent-transformer-2019-111"><a href="#HighWay-Recurrent-Transformer：Learning-multi-level-information-for-dialogue-response-selection-by-highway-recurrent-transformer-2019-111" class="headerlink" title="HighWay Recurrent Transformer：Learning multi-level information for dialogue response selection by highway recurrent transformer[2019, 111])"></a>HighWay Recurrent Transformer：Learning multi-level information for dialogue response selection by highway recurrent transformer<a href="https://arxiv.org/pdf/1903.08953.pdf" target="_blank" rel="noopener">[2019, 111]</a>)</h5><p>面向对话系统场景，对于对话系统而言，一个问题的答案和前文应答与前问问题可能都有关联，因此输入是多个句子序列，由于是句子序列，一方面使用传统Transformer输入可能会非常长，另一方面，完整的意思表达往往是以句子为一个粒度，因此仅使用之前的位置编码可能很难完全编码这种位置和句子粒度的空间信息，因此作者设计了 HighWay Recurrent Transformer网络，来优化这一问题。其具体框架如下：<br><img src="//liuzhiqi.github.io/draft/HighWay_Recurrent_Transformer.png" alt="bubble">  </p>
<p>首先从整体来讲，作者对Transformer Encoder进行了改进，对每一个问答段落，作者使用了一个Transformer Encoder Blocker进行句内编码，然后将encoder输出送入Highway Attention。Highway Attention类似于RNN的一个cell，由于结构类似RNN所以RNN具有的问题（如梯度爆炸，长期记忆消失等问题）Highway Attention可能也具有，因此模仿LSTM这里也加入了门的概念（参考了Highway Network，对输入和上一步状态加权），这里Highway Attention由两部分attention构成，一个是以上一轮输出作为KV，本轮输入作为Q计算状态的co-attention，另一部分是本轮输入的self-Attention，这里作者将权重计算融入了co&amp;self-attention权重计算中。具体公式可以参考链接中的原文。</p>
<h5 id="Lattice-Based-Transformer-：Lattice-based-transformer-encoder-for-neural-machine-translation-2019-111"><a href="#Lattice-Based-Transformer-：Lattice-based-transformer-encoder-for-neural-machine-translation-2019-111" class="headerlink" title="Lattice-Based Transformer ：Lattice-based transformer encoder for neural machine translation [2019, 111])"></a>Lattice-Based Transformer ：Lattice-based transformer encoder for neural machine translation <a href="https://www.aclweb.org/anthology/P19-1298.pdf" target="_blank" rel="noopener">[2019, 111]</a>)</h5><p>针对机器翻译，重构了输入与attention部分attention，将同一句不同分词利用Lattices，构成词的网络结构，并引入Lattice Positional Encoding编码位置，同时将所有可能的序列输入Transformer，进行attention 时同时考虑词间边的关系。</p>
<h5 id="Transformer-TTS-Network-：Neural-speech-synthesis-with-transformer-network-AAAI2019-113"><a href="#Transformer-TTS-Network-：Neural-speech-synthesis-with-transformer-network-AAAI2019-113" class="headerlink" title="Transformer TTS Network ：Neural speech synthesis with transformer network[AAAI2019, 113])"></a>Transformer TTS Network ：Neural speech synthesis with transformer network<a href="https://arxiv.org/pdf/1809.08895v3.pdf" target="_blank" rel="noopener">[AAAI2019, 113]</a>)</h5><p>Transformer 在Text to speech上的应用，借鉴了Tacotron2 ，decoder输入为音素。其框架如下，声音部分使用cnn进行编码。<br><img src="//liuzhiqi.github.io/draft/TTS_Transformer.png" alt="bubble">  </p>
<h5 id="Phrase-Based-Attention-ICLR2019-114"><a href="#Phrase-Based-Attention-ICLR2019-114" class="headerlink" title="Phrase-Based Attention[ICLR2019, 114])"></a>Phrase-Based Attention<a href="https://arxiv.org/pdf/1810.03444.pdf" target="_blank" rel="noopener">[ICLR2019, 114]</a>)</h5><p>之前的模型只从单个词角度进行学习，本文引入了通过Ngram引入词组，不同的n构成一系列词组向量然后使用MutiHead attention融合所有n的信息。其中对于Key的表示，作者没有直接使用投影矩阵相乘，而是使用卷积的，是query查询卷积核学出的特征，对于不同的n，作者分布使用各自的卷积核，然后得到的key拼接到一起（使用卷积时因为要设置窗口，所以可以更关注相邻的特征）。</p>
<h5 id="BERT-：Bert-Pre-training-of-deep-bidirectional-transformers-for-language-understanding-2019-115"><a href="#BERT-：Bert-Pre-training-of-deep-bidirectional-transformers-for-language-understanding-2019-115" class="headerlink" title="BERT ：Bert: Pre-training of deep bidirectional transformers for language understanding[2019, 115])"></a>BERT ：Bert: Pre-training of deep bidirectional transformers for language understanding<a href="https://www.aclweb.org/anthology/P19-1298.pdf" target="_blank" rel="noopener">[2019, 115]</a>)</h5><p>BERT在Transformer基础上，使用迁移学习方法提升效果，与其说是改进不如说是对Transformer encoder在不同场景的应用。其框架如下图，其中Trm是Transformer Blocker，可以看到BERT和GPT与ELMo对比。ELMo使用了双向LSTM，GPT则使用单向+Transformer做cell，这两种方法都暗含了状态转移与访问顺序，而BERT将Transformer扩展到了双向。<br><img src="//liuzhiqi.github.io/draft/BERT1.png" alt="bubble"><br>对于单纯将扩展为双向Transformer的架构（就如下图BERT描述的框架）实际上就是将Transformer全链接，然而这就产生了问题，在预训练时，无法正常预测下一个位置的值，因为是全联接，所以模型会看到要预测的值。因此作者引入了Mask Language Model(MLM)和Next sentence order(NSP)两种方法，具体做法是对词序列，可以给其中词汇随机替换成mask（类似完形填空），对于句子预测两个句子是不是下一句的关系，从文档生成连续句和非连续句。<br>在Fine-Tuning，可以根据不同任务设计输入序列，以及输出的应用。</p>
<h5 id="GPT-：Improving-language-understanding-by-generative-pre-training-2019-120"><a href="#GPT-：Improving-language-understanding-by-generative-pre-training-2019-120" class="headerlink" title="GPT ：Improving language understanding by generative pre-training [2019, 120])"></a>GPT ：Improving language understanding by generative pre-training <a href="https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf" target="_blank" rel="noopener">[2019, 120]</a>)</h5><p>GPT模型在BERT框架中有提及，其也是对transformers的改进与应用，和bert一样只是用了encoder部分做网络cell，但因为整体模型仍采用自回归模式，即新位置的学习只使用当前位置之前的输入，所以encoder部分要加入mask（又或者说，使用了去掉encoder attention sublayer的decoder）</p>
<h5 id="GPT-2-：-Language-models-are-unsupervised-multitask-learners-OpenAI2019-116"><a href="#GPT-2-：-Language-models-are-unsupervised-multitask-learners-OpenAI2019-116" class="headerlink" title="GPT-2 ： Language models are unsupervised multitask learners[OpenAI2019, 116])"></a>GPT-2 ： Language models are unsupervised multitask learners<a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" target="_blank" rel="noopener">[OpenAI2019, 116]</a>)</h5><p>GPT-2依然沿用GPT单向transformer的模式，只不过做了一些改进与改变。首先 GPT-2去掉了fine-tuning层，不再针对不同任务分别进行微调建模。而因为去掉了fine-tuning层，为了能识别特定问题的目的，其做了如下改动：</p>
<ol>
<li>首先需要增加数据集，GPT-2收集了更加广泛、数量更多的语料组成高质量文本数据集，该数据集包含800万个网页，大小为40G。</li>
<li>增加网络参数，GPT-2将Transformer堆叠的层数增加到48层，隐层的维度为1600，参数量更是达到了15亿。</li>
<li>调整LN：将layer normalization放到每个sub-block之前，并在最后一个Self-attention后再增加一个layer normalization。</li>
<li>加入任务引导提示词，如：“TL;DR:”，GPT-2模型就会知道是做摘要工作了，输入的格式就是文本+提示词，从而能自动学出目标。</li>
</ol>
<h5 id="GPT-3-：Language-models-are-few-shot-learners-OpenAI2020-117"><a href="#GPT-3-：Language-models-are-few-shot-learners-OpenAI2020-117" class="headerlink" title="GPT-3 ：Language models are few-shot learners[OpenAI2020, 117])"></a>GPT-3 ：Language models are few-shot learners<a href="https://www.aclweb.org/anthology/P19-1298.pdf" target="_blank" rel="noopener">[OpenAI2020, 117]</a>)</h5><p>GPT-3在GPT2的基础上再次扩张数据集（45TB）与参数（1750亿），同时改进了attention，当输入过多时，加入alternating dense和locally banded sparse attention，增强局部视野。</p>
<h5 id="Image-Transformer-ICML2018-118"><a href="#Image-Transformer-ICML2018-118" class="headerlink" title="Image Transformer[ICML2018, 118])"></a>Image Transformer<a href="https://arxiv.org/pdf/1802.05751.pdf" target="_blank" rel="noopener">[ICML2018, 118]</a>)</h5><p>将语言模型迁移到了图像补全的问题，其场景非常相似，文章中图像补全问题每个像素是根据之前其他像素和状态生成的，这和语言模型生成新词场景类似，因此作者将Transformer推广到了该场景。对于图像而言，输入像素很多，因此作者使用局部自注意力，限制了attention关注范围。</p>
<h3 id="2-4-2-Graph-Attention-Networks"><a href="#2-4-2-Graph-Attention-Networks" class="headerlink" title="2.4.2 Graph Attention Networks"></a>2.4.2 Graph Attention Networks</h3><h2 id="2-4-Attention-today"><a href="#2-4-Attention-today" class="headerlink" title="2.4 Attention today"></a>2.4 Attention today</h2><p>现如今使用混合模型逐渐成为深度学习领域使用Attention的主要的发展方向。基于Transformer，GATs，MemNet的工作被不断翻新并使用在各个应用中。<br>双曲空间：Hyperbolic Attention Networks (HAN) ，Hyperbolic Graph Attention Networks (GHN) 通过将研究空间从欧式空间转换到双曲空间，从而解决了数据与embedding空间指数扩张的问题。同时双去空间用于深度学习的研究从2019年开始也逐渐成为一个新的热点领域。<br>Graph Attention：从2019年，GATs在图上应用attention受到了广泛的关注，应用attention，模型可以学习到更加复杂的关系。同时其他的研究如MGNs 和 TGNs 使用了memory模块做结合也有不错的效果。</p>
<p>对RNN结构的探索：到2020年底，仍有两个工作在关注RNN结构的探索，其中一个是通过计算得到RNN的cell长度，即ACT（adaptive computation time ），ACT最开始出现于2016年，由DeepMind提出，其将RNN一个时刻的单个Cell该进成同时刻多cell迭代\(h _t ^l =Cell(h _t ^{l-1}, x _t ^l ) \)，和多层RNN不同，每层输入都为x，为了加入迭代轮数信息，没多增加一次迭代，输入x会叠加一个delta值（这里使用10标记位），再通过每轮输出+1层网络计算权重，当大于某个权重时，停止计算。而最新的DACT在ACT基础上做了改进，提出了全程可微分的E2E模型。另一个则为关注top-down 和 bottom-up在RNN传递中的作用其文章如下：</p>
<h5 id="Learning-to-combine-top-down-and-bottom-up-signals-in-recurrent-neural-networks-with-attention-over-modules-ICML2020-125"><a href="#Learning-to-combine-top-down-and-bottom-up-signals-in-recurrent-neural-networks-with-attention-over-modules-ICML2020-125" class="headerlink" title="Learning to combine top-down and bottom-up signals in recurrent neural networks with attention over modules[ICML2020, 125])"></a>Learning to combine top-down and bottom-up signals in recurrent neural networks with attention over modules<a href="https://arxiv.org/pdf/2006.16981.pdf" target="_blank" rel="noopener">[ICML2020, 125]</a>)</h5><p>Bottom-up指直接从句子中观测到的信息，top-down则指基于过往经验和short-term memory得到的预测。针对这两种情况作者设计了如下框架：<br><img src="//liuzhiqi.github.io/draft/top-down_bottom-up.png" alt="bubble"><br>从整体来看，其框架是对RNN进行了改进，特征除了从底层传递到上层，从t-1时刻传递到t时刻外，还加入了从t-1时刻到上层传递到t时刻下层的边（即引入了作者所谓的TopDown的考量）。<br>对于一层而言，作者使用了Recurrent Independent Mechanisms，和传统rnn不同，一个rnn层是多个独立的cell共同组成。<br>对于这些cell，一方面做着设计了Selective Activation，将t-1的多个cell输出做query，零向量和cell的输入[Zero,xt] 做KV，进行attention，这样可以得到输入和零向量的attention评分，从而得到该层与输入的关系，利用评分选出m个关系最大的cell，参与状态更新，其余的保持t-1状态。<br>另一方面，作者还设计了cell间的Communication，这主要用当前时刻cell的输出做self-attention，融合彼此间特征，然后将融合后的结果叠加到当前输出（只有Selective Activation激活的cell需要更新）。<br>对于层间传递而言，和LSTM这一类不同，作者没有仅将上一层做输入，而是使用了当前层t-1时刻状态 \(h _{t-1} ^l\) 做Query，\([ \emptyset, h _t ^{l-1}, h _{t-1} ^{l+1}]\)做KV，从而融合上一层与下一层的状态，这里同样引入了0向量，可以很好的减轻当 \( h _t ^{l-1}\)  与\( h _{t-1} ^{l+1}\) 和当前层上一时刻状态无关时，多余的特征引入。</p>
<p><a name="GATs"> </a><br>**Transformer：Attention is all you need<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">[Google2017, 37]</a>**：</p>

        
      </div>
      
      
      
    </div>
    
    
    
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpeg" alt="Zhiqi Liu">
            
              <p class="site-author-name" itemprop="name">Zhiqi Liu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/liuzhiqi" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:yourname@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#3-Attention-Mechanisms"><span class="nav-number">1.</span> <span class="nav-text">3 Attention Mechanisms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-1-Transformer-Based"><span class="nav-number">1.0.1.</span> <span class="nav-text">2.4.1 Transformer Based</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-1-1-Transformer方法的改进"><span class="nav-number">1.0.1.1.</span> <span class="nav-text">2.4.1.1 Transformer方法的改进</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Weighted-Transformer-：Weighted-transformer-network-for-machine-translation-2017-102"><span class="nav-number">1.0.1.1.1.</span> <span class="nav-text">Weighted Transformer ：Weighted transformer network for machine translation[2017, 102])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Star-transformer-NAACL2019-103"><span class="nav-number">1.0.1.1.2.</span> <span class="nav-text">Star-transformer[NAACL2019, 103])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Self-Attention-with-Relative-Position-Representations-NAACL2018-174"><span class="nav-number">1.0.1.1.3.</span> <span class="nav-text">Self-Attention with Relative Position Representations[NAACL2018, 174])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Sparse-Transformers-：Generating-Long-Sequences-with-Sparse-Transformers-OpenAl2019-103"><span class="nav-number">1.0.1.1.4.</span> <span class="nav-text">Sparse Transformers ：Generating Long Sequences with Sparse Transformers[OpenAl2019, 103])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Set-transformer-A-framework-for-attention-based-permutation-invariant-neural-networks-ICML2019-106"><span class="nav-number">1.0.1.1.5.</span> <span class="nav-text">Set transformer: A framework for attention-based permutation-invariant neural networks.[ICML2019, 106])</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-1-2-Transformer方法的应用"><span class="nav-number">1.0.1.2.</span> <span class="nav-text">2.4.1.2 Transformer方法的应用</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Doubly-attentive-transformer-machine-translation-2018-107"><span class="nav-number">1.0.1.2.1.</span> <span class="nav-text">Doubly attentive transformer machine translation.[2018, 107])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Input-Combination-Strategies-for-Multi-Source-Transformer-Decoder-2018-106"><span class="nav-number">1.0.1.2.2.</span> <span class="nav-text">Input Combination Strategies for Multi-Source Transformer Decoder[2018, 106])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Input-Combination-Strategies-for-Multi-Source-Transformer-Decoder-2019-106"><span class="nav-number">1.0.1.2.3.</span> <span class="nav-text">Input Combination Strategies for Multi-Source Transformer Decoder[2019, 106])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#HighWay-Recurrent-Transformer：Learning-multi-level-information-for-dialogue-response-selection-by-highway-recurrent-transformer-2019-111"><span class="nav-number">1.0.1.2.4.</span> <span class="nav-text">HighWay Recurrent Transformer：Learning multi-level information for dialogue response selection by highway recurrent transformer[2019, 111])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Lattice-Based-Transformer-：Lattice-based-transformer-encoder-for-neural-machine-translation-2019-111"><span class="nav-number">1.0.1.2.5.</span> <span class="nav-text">Lattice-Based Transformer ：Lattice-based transformer encoder for neural machine translation [2019, 111])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Transformer-TTS-Network-：Neural-speech-synthesis-with-transformer-network-AAAI2019-113"><span class="nav-number">1.0.1.2.6.</span> <span class="nav-text">Transformer TTS Network ：Neural speech synthesis with transformer network[AAAI2019, 113])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Phrase-Based-Attention-ICLR2019-114"><span class="nav-number">1.0.1.2.7.</span> <span class="nav-text">Phrase-Based Attention[ICLR2019, 114])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#BERT-：Bert-Pre-training-of-deep-bidirectional-transformers-for-language-understanding-2019-115"><span class="nav-number">1.0.1.2.8.</span> <span class="nav-text">BERT ：Bert: Pre-training of deep bidirectional transformers for language understanding[2019, 115])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#GPT-：Improving-language-understanding-by-generative-pre-training-2019-120"><span class="nav-number">1.0.1.2.9.</span> <span class="nav-text">GPT ：Improving language understanding by generative pre-training [2019, 120])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#GPT-2-：-Language-models-are-unsupervised-multitask-learners-OpenAI2019-116"><span class="nav-number">1.0.1.2.10.</span> <span class="nav-text">GPT-2 ： Language models are unsupervised multitask learners[OpenAI2019, 116])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#GPT-3-：Language-models-are-few-shot-learners-OpenAI2020-117"><span class="nav-number">1.0.1.2.11.</span> <span class="nav-text">GPT-3 ：Language models are few-shot learners[OpenAI2020, 117])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Image-Transformer-ICML2018-118"><span class="nav-number">1.0.1.2.12.</span> <span class="nav-text">Image Transformer[ICML2018, 118])</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-2-Graph-Attention-Networks"><span class="nav-number">1.0.2.</span> <span class="nav-text">2.4.2 Graph Attention Networks</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-Attention-today"><span class="nav-number">1.1.</span> <span class="nav-text">2.4 Attention today</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Learning-to-combine-top-down-and-bottom-up-signals-in-recurrent-neural-networks-with-attention-over-modules-ICML2020-125"><span class="nav-number">1.1.0.0.1.</span> <span class="nav-text">Learning to combine top-down and bottom-up signals in recurrent neural networks with attention over modules[ICML2020, 125])</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhiqi Liu</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
